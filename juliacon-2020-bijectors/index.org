#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t toc:nil
#+OPTIONS: header-args:julia :session jl
#+EXCLUDE_TAGS: noexport

#+REVEAL_ROOT: file:///home/tor/Projects/mine/presentations/cambridge-julia-meetup/assets/reveal.js-3.8.0/
#+REVEAL_MATHJAX_URL: file:///home/tor/Projects/mine/presentations/cambridge-julia-meetup/assets/MathJax-2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+REVEAL_TITLE_SLIDE: <div><div style="margin: -200px auto; opacity: 0.2;"><p><object data="https://turing.ml/dev/assets/images/turing-logo-wide.svg"></object></p></div><h1><code>Bijectors.jl</code></h1><h3>Transforming probability distributions in Julia</h3><p><a href="https://github.com/TuringLang/Bijectors.jl">https://github.com/TuringLang/Bijectors.jl</a></p></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: moon

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: =Bijectors.jl=: Transforming probability distributions in Julia

* Time                                                             :noexport:
1. 3min
2. 3min
3. 2min


* A "bijector" you say?

#+REVEAL: split

#+name: def:bijector
#+begin_definition :title ""
A *bijector* or *diffeomorphism* is a differentiable /bijection/ (one-to-one and onto) $b$ with a /differentiable/ inverse $b^{-1}$.
#+end_definition

#+ATTR_REVEAL: :frag (appear)
For example $b(x) = \exp(x)$ for $x \in (-\infty, \infty)$
#+ATTR_REVEAL: :frag (appear)
- $\exp$ is differentiable
- $\exp$ has inverse $\log$
- $\log$ is differentiable (on $(0, \infty)$)
#+ATTR_REVEAL: :frag (appear)
So $\exp$ (and $\log$) is a bijector!

#+REVEAL: split

#+begin_quote
Ok, but why do I care?
#+end_quote

#+HTML: <div class="fragment (appear)">
Consider a generative process for $y$ defined
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}
where $b$ is some transformation.
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
If $b$ is a bijector, this /induces/ a density $\tilde{p}(y)$ defined by
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Useful for changing the support of distributions, reparameterization, density estimation, varational inference, etc.
#+HTML: </div>

* Setup                                                            :noexport:
#+begin_src jupyter-julia :exports both
versioninfo()
#+end_src

#+RESULTS:
: Julia Version 1.4.2
: Commit 44fa15b150* (2020-05-23 18:35 UTC)
: Platform Info:
:   OS: Linux (x86_64-pc-linux-gnu)
:   CPU: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
:   WORD_SIZE: 64
:   LIBM: libopenlibm
:   LLVM: libLLVM-8.0.1 (ORCJIT, skylake)

* =Bijectors.jl=
:PROPERTIES:
:header-args: :exports both
:END:

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports both
using Bijectors; using Bijectors: Exp, Log

b = Exp()
b⁻¹ = inv(b)

b⁻¹ isa Log
#+end_src

#+RESULTS:
: true

#+HTML: <div class="fragment (appear)">
We can evaluate a =Bijector=

#+begin_src jupyter-julia :session jl :exports both
x = 0.0
b(x) == 1.0  # since e⁰ = 1
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We can /compose/ bijectors to get a new =Bijector=

#+begin_src jupyter-julia :session jl :exports both
(b ∘ b) isa Bijector
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

And evaluate compositions of bijectors

#+begin_src jupyter-julia :session jl :exports both
(b⁻¹ ∘ b)(x) == x
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
What about more complex/deeper compositions?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
cb = b ∘ b ∘ b
cb⁻¹ = inv(cb)        # <= inversion of a "large" composition

(cb⁻¹ ∘ cb)(x) == x
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
As mentioned before, the following terms are of particular interest
\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Which works seamlessly even for compositions

#+begin_src jupyter-julia :session jl :exports both
logabsdetjac(cb, x)
#+end_src

#+RESULTS:
: 3.718281828459045

#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :exports none
function Bijectors.logabsdetjac(cb::Composed{T}, x) where {T<:Tuple}
    N = length(T.parameters)

    expr = Expr(:block)
    push!(expr.args, :((y, logjac) = forward(cb.ts[1], x)))

    for i = 2:N - 1
        temp = gensym(:res)
        push!(expr.args, :($temp = forward(cb.ts[$i], y)))
        push!(expr.args, :(y = $temp.rv))
        push!(expr.args, :(logjac += $temp.logabsdetjac))
    end
    # don't need to evaluate the last bijector, only it's `logabsdetjac`
    push!(expr.args, :(logjac += logabsdetjac(cb.ts[$N], y)))

    push!(expr.args, :(return logjac))

    return expr
end
#+end_src

Above =logabsdetjac(cb, x)= expands to

#+begin_src jupyter-julia :exports code :eval no
(y, logjac) = forward(cb.ts[1], x)
var"##res#256" = forward(cb.ts[2], y)
y = (var"##res#256").rv
logjac += (var"##res#256").logabsdetjac
logjac += logabsdetjac(cb.ts[3], y)
return logjac
#+end_src

#+HTML: <div class="fragment (appear)">
while if we let =cb = Composed([b, b, b])=, we get

#+begin_src jupyter-julia :exports code :eval no
y, logjac = forward(cb.ts[1], x)
for i = 2:length(cb.ts)
    res = forward(cb.ts[i], y)
    y = res.rv
    logjac += res.logabsdetjac
end

return logjac
#+end_src

And similarily for other methods.
#+HTML: </div>

#+begin_src jupyter-julia :exports none
@generated function Bijectors.logabsdetjac(cb::Composed{T}, x) where {T<:Tuple}
    N = length(T.parameters)

    expr = Expr(:block)
    push!(expr.args, :((y, logjac) = forward(cb.ts[1], x)))

    for i = 2:N - 1
        temp = gensym(:res)
        push!(expr.args, :($temp = forward(cb.ts[$i], y)))
        push!(expr.args, :(y = $temp.rv))
        push!(expr.args, :(logjac += $temp.logabsdetjac))
    end
    # don't need to evaluate the last bijector, only it's `logabsdetjac`
    push!(expr.args, :(logjac += logabsdetjac(cb.ts[$N], y)))

    push!(expr.args, :(return logjac))

    return expr
end
#+end_src

#+REVEAL: split

#+begin_src jupyter-julia
d = Exponential(1.)
#+end_src

#+RESULTS:
: Exponential{Float64}(θ=1.0)

#+begin_src jupyter-julia 
support(d)
#+end_src

#+RESULTS:
: RealInterval(0.0, Inf)

#+HTML: <div class="fragment (appear)">

#+begin_src jupyter-julia 
b = bijector(d)
#+end_src

#+RESULTS:
: Log{0}()

#+begin_src jupyter-julia 
td = transformed(d, b) # OR `transformed(d)` in this case
#+end_src

#+RESULTS:
: Bijectors.TransformedDistribution{Exponential{Float64},Log{0},Univariate}(
: dist: Exponential{Float64}(θ=1.0)
: transform: Log{0}()
: )
: 

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia 
support(td)
#+end_src

#+RESULTS:
: RealInterval(-Inf, Inf)

#+HTML: <div style="font-size: 0.5em;">
*Note:* =Distributions.support= only works for =UnivariateDistribution=.
#+HTML: </div>

#+HTML: </div>

#+REVEAL: split

|-------------------------------------------------------------+-------------------------+--------------|
| Operation                                                   | Method                  | Freebie    |
|-------------------------------------------------------------+-------------------------+--------------|
| $b \mapsto b^{-1}$                                          | =inv(b)=                | $\checkmark$ |
| $(b_1, b_2) \mapsto (b_1 \circ b_2)$                        | =b1 ∘ b2=               | $\checkmark$ |
| $(b_1, b_2) \mapsto [b_1, b_2]$                             | =stack(b1, b2)=         | $\checkmark$ |
|-------------------------------------------------------------+-------------------------+--------------|

#+REVEAL: split

|----------------------------------------------------------------+----------------------+--------------|
| Operation                                                      | Method               | Freebie    |
|----------------------------------------------------------------+----------------------+--------------|
| $x \mapsto b(x)$                                               | =b(x)=               | $\times$     |
| $y \mapsto b^{-1}(y)$                                          | =inv(b)(y)=          | $\times$     |
| $x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert$         | =logabsdetjac(b, x)= | AD           |
| $x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)$ | =forward(b, x)=      | $\checkmark$ |
|----------------------------------------------------------------+----------------------+--------------|

#+REVEAL: split

|--------------------------------------------------------------------------------+-------------------------+--------------|
| Operation                                                                      | Method                  | Freebie    |
|--------------------------------------------------------------------------------+-------------------------+--------------|
| $p \mapsto q:= b_* p$                                                          | =q = transformed(p, b)= | $\checkmark$ |
| $y \sim q$                                                                     | =y = rand(q)=           | $\checkmark$ |
| $\log q(y)$                                                                    | =logpdf(q, y)=          | $\checkmark$ |
| $p \mapsto b$ s.t. $\mathrm{support}(b_* p) = \mathbb{R}^d$                    | =bijector(p)=           | $\times$ |
| $\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)$ | =forward(q)=            | $\checkmark$ |
|--------------------------------------------------------------------------------+-------------------------+--------------|


** Implementing a =Bijector= :noexport:
#+begin_src jupyter-julia :session jl :eval no :exports code
using StatsFuns: logit, logistic

struct Logit{T<:Real} <: Bijector{0} # <= 0-dimensional, i.e. expects `Real` input (or `Vector` which is treated as batch)
    a::T
    b::T
end

(b::Logit)(x) = @. logit((x - b.a) / (b.b - b.a))
# `orig` contains the `Bijector` which was inverted
(ib::Inversed{<:Logit})(y) = @. (ib.orig.b - ib.orig.a) * logistic(y) + ib.orig.a

logabsdetjac(b::Logit, x) = @. - log((x - b.a) * (b.b - x) / (b.b - b.a))
#+end_src

#+REVEAL: split

#+begin_src jupyter-julia :session jl :eval no
julia> b = Logit(0.0, 1.0)
Logit{Float64}(0.0, 1.0)

julia> y = b(0.6)
0.4054651081081642

julia> inv(b)(y)
0.6

julia> logabsdetjac(b, 0.6)
1.4271163556401458

julia> logabsdetjac(inv(b), y) # defaults to `- logabsdetjac(b, inv(b)(x))`
-1.4271163556401458

julia> forward(b, 0.6)         # defaults to `(rv=b(x), logabsdetjac=logabsdetjac(b, x))`
(rv = 0.4054651081081642, logabsdetjac = 1.4271163556401458)
#+end_src


* Example: density estimation

#+REVEAL: split

Consider an =Affine= transformation, i.e.
\begin{equation*}
\mathrm{aff}(x) = W x + b
\end{equation*}
for matrix $W$ (with assumption $\det W \ne 0$) and vector $b$,
#+HTML: <div class="fragment (appear)">
and a non-linear (but /invertible/) activation function, e.g. =LeakyReLU=:
\begin{equation*}
a(x) = 
\begin{cases}
  x & \text{if } x \ge 0 \\
  \alpha x & \text{if } x < 0
\end{cases}
\end{equation*}
for some /non-zero/ $\alpha \in \mathbb{R}$ (usually chosen to be very small).
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Looks familiar?

#+HTML: <div class="fragment (appear)">
Yup; it's basically an _invertible neural network_!

#+begin_src jupyter-julia :session jl
layers = [LeakyReLU(α[i]) ∘ Affine(W[i], b[i]) for i = 1:num_layers]

b = foldl(∘, layers)
td = transformed(base_dist, b)  # <= "deep" normalising flow!
#+end_src

#+HTML: </div>

#+REVEAL: split

#+ATTR_HTML: :width 35%
#+CAPTION: Empirical density estimate (blue) compared with single batch of samples (red). Code can be found in =scripts/nf_banana.jl=.
file:figures/nf-banana-density-estimation.gif

* Thank you!

#+begin_export HTML
<div id="hidden" style="display:none;">
  <div id="header">
    <!-- <div id="header-left">Tor Erlend Fjelde</div> -->
    <!-- <div id="header-right">HEADER-RIGHT</div> -->
    <div id="footer">
      <div>
        <div id="footer-logo">
          <!-- <img src="figures/juliacon2020-background.png" /> -->
          <!-- <img src="figures/julia-logo-dark.svg" /> -->
          <!-- <img src="figures/juliacon.svg" /> -->
          <strong>
            JuliaCon
            <sup>2020</sup>
          </strong>
        </div>
      </div>
      
      <div>
        <!-- <img src="figures/juliacon.svg" /> -->
      </div>
      
      <div></div>
    </div>
  </div>
</div>

<script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
<script type="text/javascript">
  // 3. On Reveal.js ready event, copy header/footer <div> into each `.slide-background` <div>
      var header = $('#header').html();
      if ( window.location.search.match( /print-pdf/gi ) ) {
      Reveal.addEventListener( 'ready', function( event ) {
      $('.slide-background').append(header);
      });
      }
      else {
      $('div.reveal').append(header);
      }
</script>
#+end_export
