#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_TITLE_SLIDE: <div><h1>You have data and I have distributions</h1><h3>A talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></h3><div style="margin: -200px auto; opacity: 0.2;"><p><object data="https://turing.ml/dev/assets/images/turing-logo-wide.svg"></object></p></div></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: moon

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: You have data and I have distributions: a talk on =Turing.jl= and =Bijectors.jl=

* Overview
1. Bayesian inference and a bit of density estimation
   - Why you want to do Bayesian inference
   - What it means to do Bayesian inference
   - Brief mention of what "density estimation" is
2. =Turing.jl= on a simple example
   - Bayesian inference
   - /Approximate/ Bayesian inference (variational inference)
3. =Bijectors.jl=:
   - What it's about
   - Why it's neat
   - Normalizing flow
4. Normalizing flow ADVI
5. Combining /everything/

* Bayesian inference and such
#+ATTR_REVEAL: :frag (appear)
- Have some dataset $\left\{ x_i \right\}_{i = 1}^n$
- Believe data $\left\{ x_i \right\}_{i = 1}^n$ was generated by some process and we're interested in inferring parameters $\theta$ of this process
  - E.g. in linear regression you have data $\left\{ (x_i, y_i) \right\}_{i = 1}^n$ and want to infer coefficients $\theta := \beta$

#+REVEAL: split

_Frequentist_
#+begin_quote
A single point as the estimate of $\theta$ is good enough.
#+end_quote

#+HTML: <div class="fragment (appear)">

_Bayesian_ 
#+begin_quote
But we have _finite_ data! And this dataset just happen to give you the one and only $\theta$?!

No, no, no, we need a distribution over the $\theta$ given the data (a *posterior*):
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big)
\end{equation*}
#+end_quote

#+HTML: </div>

** Bayes' rule
*Bayes' rule* gives us
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big) = \frac{p\big(\left\{ x_i \right\}_{i = 1}^n \mid \theta\big) p(\theta)}{p\big(\left\{ x_i \right\}_{i = 1}^n\big)}
\end{equation*}
#+HTML: <div class="fragment (appear)">
or, since the denominator is constant,
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big) \propto p \big( \left\{ x_i \right\}_{i = 1}^n \mid \theta \big) p(\theta)
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
For the family of inference methods known as /Markov Chain Monte-Carlo (MCMC)/, this proportional factor is all we need.
#+HTML: </div>

* Setup
#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Plots, StatsPlots
#+end_src

#+begin_src jupyter-julia :session jl :results silent :exports none :async yes
using Pkg; Pkg.activate("/home/tor/.julia/dev/Bijectors")
#+end_src

#+begin_src jupyter-julia :session jl :exports code :async yes
using Bijectors
#+end_src

#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Turing
#+end_src

_Disclaimer:_ All functionality in this talk is not yet available on the master branch of =Turing.jl= and =Bijectors.jl=, but should be soon™.


* =Turing.jl=
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
=Turing.jl= is a (universal) _probabilistic programming language_ (PPL) in Julia.
#+end_quote

#+ATTR_REVEAL: :frag (appear)
What does that even mean?

#+ATTR_REVEAL: :frag (appear)
1. Specify generative model in Julia with neat syntax
2. Bayesian inference to estimate posterior $p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$ using a vast library of MCMC samplers
3. ???
4. Profit (in expectation)!!!

** Example: Gaussian-InverseGamma conjugate model
#+ATTR_REVEAL: :frag (appear)
In mathematical notation:
#+ATTR_REVEAL: :frag (appear)
\begin{equation*}
\begin{split}
  s & \sim \mathrm{InverseGamma}(2, 3) \\
  m & \sim \mathcal{N}(0, \sqrt{s}) \\
  x_i & \sim \mathcal{N}(m, \sqrt{s}), \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+ATTR_REVEAL: :frag (appear)
In =Turing.jl=:
#+ATTR_REVEAL: :frag (appear)
#+begin_src jupyter-julia :session jl :exports code
@model model(x) = begin
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, √s)
    for i = 1:length(x)
        x[i] ~ Normal(m, √s)
    end
end
#+end_src

#+RESULTS:
: model (generic function with 2 methods)

#+REVEAL: split

Generate some fake data and instantiate the model

#+begin_src jupyter-julia :session jl :exports code
xs = randn(1_000)
m = model(xs)
#+end_src

#+HTML: <div class="fragment (appear)">

Now sample to obtain posterior $p\big(m, s \mid \left\{ x_i \right\}_{i = 1}^n \big)$

#+begin_src jupyter-julia :session jl :exports code
# Sample 1000 samples using HMC
samples_nuts = sample(m, NUTS(10_000, 200, 0.65));
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
┌ Info: Found initial step size
│   init_ϵ = 0.025
└ @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
┌ Info: Finished 200 adapation steps
│   adaptor = StanHMCAdaptor(n_adapts=200, pc=DiagPreconditioner, ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.65, state.ϵ=1.302609925324549), init_buffer=75, term_buffer=50)
│   τ.integrator = Leapfrog(ϵ=1.3)
│   h.metric = DiagEuclideanMetric([0.00132985, 0.000834938])
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
┌ Info: Finished 10000 sampling steps in 1.502001043 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.00132985, 0.000834938]))
│   τ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(ϵ=1.3), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 2115.310793041227
│   average_acceptance_rate = 0.8436075614790801
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
#+END_EXAMPLE

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Aaaand we can plot the resulting (empirical) posterior

#+begin_src jupyter-julia :session jl :exports code :file figures/gaussian-inversegamma-hmc.svg
plot(samples_nuts[[:s, :m]])
#+end_src

#+HTML: </div>

#+REVEAL: split

[[file:figures/gaussian-inversegamma-hmc.svg]]

** *Approximate* inference (Variational inference)
Might be happy with an /approximation/ to your posterior $p \big( \theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$.

#+HTML: <div class="fragment (appear)">

*Variational inference (VI)* is an approximate approach which formulates the problem as an /optimization/ problem:
#+HTML: </div>
#+HTML: <div class="fragment (appear)">
\begin{equation*}
\underset{q \in \mathscr{Q}}{\text{argmin}}\ \mathrm{D_{KL}} \big( q(\theta) \mid p(\theta \mid \left\{ x_i \right\}_{i = 1}^n ) \big) \quad \text{or} \quad \underset{q \in \mathscr{Q}}{\text{argmax}}\ \mathrm{ELBO} \big( q(\theta) \big)
\end{equation*}
where
\begin{equation*}
\mathrm{ELBO} \big( q(\theta) \big) = \mathbb{E}_{\theta \sim q(\theta)} \big[ \log p\big(\theta, \left\{ x_i \right\}_{i = 1}^n \big) \big] + \mathbb{H} \big( q(\theta) \big)
\end{equation*}

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
_Caveat:_ usually assume $\mathscr{Q}$ is the family of Gaussians with /diagonal/ covariance.

*** Automatic Differentiation Variational Inference (ADVI)
(Mean-field) ADVI is a simple but flexible VI approach that exists in =Turing.jl=
#+begin_src jupyter-julia :session jl :results none :exports code
# "Configuration" for ADVI
# - 10 samples for gradient estimation
# - Perform 15 000 optimization steps
advi = ADVI(10, 15_000)

# Perform `ADVI` on model `m` to get variational posterior `q`
q = vi(m, advi)
#+end_src

#+HTML: <div class="fragment (appear)">
To sample and compute probabilities
#+begin_src jupyter-julia :session jl :exports code
xs = rand(q, 10)
logpdf(q, xs)
#+end_src
#+HTML: </div>

#+REVEAL: split

#+CAPTION: =ADVI= applied to the Normal-InverseGamma generative model from earlier. _Disclaimer:_ this plot is generated by writing the optimization loop myself rather than using the simple =vi(m, advi)= call.
[[./figures/advi_w_elbo_fps15_125_forward_diff.gif]]

** Benchmarks of =HMC=
file:figures/turing-benchmarks.png

* =Bijectors.jl=
#+ATTR_REVEAL: :frag (appear)
#+name: def:bijector
#+begin_definition :title ""
A *bijector* or *diffeomorphism* is a differentiable /bijection/ $b$ with a /differentiable/ inverse $b^{-1}$.
#+end_definition

#+ATTR_REVEAL: :frag (appear)
For example $b(x) = \exp(x)$
#+ATTR_REVEAL: :frag (appear)
- $\exp$ is differentiable
- $\exp$ has inverse $\log$
- $\log$ is differentiable (on $(0, \infty)$)
#+ATTR_REVEAL: :frag (appear)
So $\exp$ (and $\log$) is a bijector!

#+REVEAL: split

In =Bijectors.jl=

#+begin_src jupyter-julia :session jl :exports both :results none
using Bijectors; using Bijectors: Exp, Log

b = Exp()
b⁻¹ = inv(b)

b⁻¹ isa Log
#+end_src

#+RESULTS:
: true

#+HTML: <div class="fragment (appear)">
We can evaluate a =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
x = 0.0
b(x) == 1.0  # since e⁰ = 1
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We can /compose/ bijectors to get a new =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
(b ∘ b) isa Bijector
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

And evaluate compositions of bijectors

#+begin_src jupyter-julia :session jl :exports both
(b⁻¹ ∘ b)(x) == x
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
What about more complex/deeper compositions?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
cb = b ∘ b ∘ b
cb⁻¹ = inv(cb)        # <= inversion of a "large" composition

(cb⁻¹ ∘ cb)(x) == x
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We'll see later that of particular interest is the term

\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Which works seamlessly even for compositions

#+begin_src jupyter-julia :session jl :exports both
logabsdetjac(cb, x)
#+end_src

#+RESULTS:
: 3.718281828459045

#+HTML: </div>

** How does it relate to distributions?
#+HTML: <div class="fragment (appear)">
Consider
\begin{equation*}
x \sim \mathcal{N}(1, 5)
\end{equation*}
#+HTML: </div>
#+HTML: <div class="fragment (appear)">
Or, equivalently,
\begin{equation*}
\begin{split}
  \xi &\sim \mathcal{N}(0, 1) \\
  x &:= 1 + 5 \xi
\end{split}
\end{equation*}
#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports none :results none
using Random; Random.seed!(42)
#+end_src

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-pdfs.svg :results none
d1 = Normal(1.0, 5.0) # 𝒩(1, 5)
xs = rand(d1, 100_000)

d2 = Normal(0.0, 1.0) # 𝒩(0, 1)
ξ = rand(d2, 100_000)
ys = 1.0 .+ ξ .* 5.0  # y ~ 𝒩(1, 5)

density(xs, label = "d1", linewidth = 3)
density!(ys, label = "d2", linewidth = 3)
#+end_src

[[file:figures/normal-pdfs.svg]]

#+REVEAL: split

Bijectors are transformations which _induce_ distributions

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both :exports code
using Bijectors: Shift, Scale

# Define the transform
b = Shift(1.0) ∘ Scale(5.0) # => x ↦ 1.0 + 5.0x
td = transformed(d2, b)     # => 𝒩(1.0, 5.0)
#+end_src

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Moreover, =td= is a =TransformedDistribution= /and/

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
td isa Distribution
#+end_src

#+RESULTS:
: true

Yay!

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src jupyter-julia :session jl :exports both
y = rand(td)
# Ensure we have the same densities
logpdf(td, y) ≈ logpdf(d1, y)
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-transformed-pdfs.svg :results none
x_range = -14.0:0.05:16.0
plot(x_range, x -> pdf(d1, x), label = "d1", linewidth = 4, linestyle = :dash)
plot!(x_range, x -> pdf(td, x), label = "td", linewidth = 4, alpha = 0.6)
#+end_src

#+CAPTION: Density of $\mathcal{N}(0, 1)$ transformed by $\xi \mapsto 1 + 5\xi$ (labeled =td=) and $\mathcal{N}(1, 5)$ (labeled =d1=).
[[file:figures/normal-transformed-pdfs.svg]]

#+REVEAL: split

#+begin_quote
Great; just another way to draw samples from a normal distribution...
#+end_quote

#+ATTR_REVEAL: :frag (appear)
Or /is it/?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports none
using Random; Random.seed!(10);
#+end_src

#+RESULTS:

#+begin_src jupyter-julia :session jl :exports code :file figures/simple-nf-pdf.svg :results none
d = MvNormal(zeros(1), ones(1))

# "Deep" transformation
b = (
    RadialLayer(1) ∘ RadialLayer(1) ∘
    RadialLayer(1) ∘ RadialLayer(1)
)
td = transformed(d, b)

# Sample from flow
xs = rand(td, 10_000)

x_range = minimum(xs):0.05:maximum(xs)
lps = pdf(td, reshape(x_range, (1, :)))  # compute the probabilities

histogram(vec(xs), bins = 100; normed = true, label = "", alpha = 0.7)
xlabel!("y")
plot!(x_range, lps, linewidth = 3, label = "p(y)")
#+end_src

#+HTML: </div>

#+REVEAL: split
#+LaTeX: \clearpage

#+RESULTS:
[[file:figures/simple-nf-pdf.svg]]

That doesn't look very /normal/, now does it?!

*** How?
It turns out that if $b$ is a =Bijector=, the process
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}
/induces/ a density $\tilde{p}(y)$ defined by
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}

#+HTML: <div class="fragment (appear)">
Therefore if we can compute $\mathcal{J}_{b^{-1}}(y)$ we can indeed compute $\tilde{p}(y)$!
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
But is this actually useful?
#+end_quote

** Normalising flows: parameterising $b$
One might consider constructing a /parameterised/ =Bijector= $b_{\phi}$.

#+HTML: <div class="fragment (appear)">
Given a density $p(x)$ we can obtain a parameterised density
\begin{equation*}
\tilde{p}_{\phi}(y) = p \big( b_{\phi}^{-1}(y) \big) \left| \det \mathcal{J}_{b_{\phi}^{-1}}(y) \right|
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
$b_{\phi}$ is often referred to as a *normalising flow (NF)*
#+end_quote

#+HTML: <div class="fragment (appear)">
Can now optimise any objective over distributions, e.g. perform /maximum likelihood estimation/ (MLE) for some given i.i.d. dataset $\left\{ y_i \right\}_{i = 1}^n$
\begin{equation*}
\underset{\phi}{\text{argmax}}\ \sum_{i = 1}^{n} \tilde{p}_{\phi}(y_i)
\end{equation*}
#+HTML: </div>

*** Example: MLE using NF
Consider an =Affine= transformation, i.e.
\begin{equation*}
\mathrm{aff}(x) = W x + b
\end{equation*}
for matrix $W$ and vector $b$,
#+HTML: <div class="fragment (appear)">
and a non-linear activation function, e.g. =LeakyReLU=
\begin{equation*}
a(x) = 
\begin{cases}
  x & \text{if } x \ge 0 \\
  \alpha x & \text{if } x < 0
\end{cases}
\end{equation*}
for some /non-zero/ $\alpha \in \mathbb{R}$ (usually chosen to be very small).
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Can define a "deep" NF by composing such transformations! Looks familiar?

#+HTML: <div class="fragment (appear)">
Yup; it's basically an invertible neural network! (assuming $\det W \ne 0$)

#+begin_src jupyter-julia :session jl
layers = [LeakyReLU(α[i]) ∘ Affine(W[i], b[i]) for i = 1:num_layers]

b = foldl(∘, layers)
td = transformed(base_dist, b)  # <= "deep" normalizing flow!
#+end_src

#+HTML: </div>

#+REVEAL: split

#+ATTR_HTML: :width 35%
#+CAPTION: Empirical density estimate (blue) compared with single batch of samples (red).
file:figures/nf-banana-density-estimation.gif

** Methods

|-------------------------------------------------------------+-------------------------+--------------|
| Operation                                                   | Method                  | Automatic    |
|-------------------------------------------------------------+-------------------------+--------------|
| $b \mapsto b^{-1}$                                          | =inv(b)=                | $\checkmark$ |
| $(b_1, b_2) \mapsto (b_1 \circ b_2)$                        | =b1 ∘ b2=               | $\checkmark$ |
| $(b_1, b_2) \mapsto [b_1, b_2]$                             | =stack(b1, b2)=         | $\checkmark$ |
| $(b, n) \mapsto b^n := b \circ \cdots \circ b$ (n times)    | =b^n=                   | $\checkmark$ |
|-------------------------------------------------------------+-------------------------+--------------|

#+REVEAL: split

|----------------------------------------------------------------+----------------------+--------------|
| Operation                                                      | Method               | Automatic    |
|----------------------------------------------------------------+----------------------+--------------|
| $x \mapsto b(x)$                                               | =b(x)=               | $\times$     |
| $y \mapsto b^{-1}(y)$                                          | =inv(b)(y)=          | $\times$     |
| $x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert$         | =logabsdetjac(b, x)= | AD           |
| $x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)$ | =forward(b, x)=      | $\checkmark$ |
|----------------------------------------------------------------+----------------------+--------------|

#+REVEAL: split

|--------------------------------------------------------------------+-------------------------+--------------|
| Operation                                                          | Method                  | Automatic    |
|--------------------------------------------------------------------+-------------------------+--------------|
| $p \mapsto q:= b_* p$                                              | =q = transformed(p, b)= | $\checkmark$ |
| $y \sim q$                                                         | =y = rand(q)=           | $\checkmark$ |
| $p \mapsto b$ s.t. $\mathrm{support}(b_* p) = \mathbb{R}^d$        | =bijector(p)=           | $\checkmark$ |
| $\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)$ | =forward(q)=            | $\checkmark$ |
|--------------------------------------------------------------------+-------------------------+--------------|


* NF-ADVI vs. MF-ADVI
#+HTML: <div class="fragment (appear)">

Consider $L = \begin{pmatrix} 10 & 0 \\ 10 & 10 \end{pmatrix}$ and
\begin{equation*}
\begin{split}
  m & \sim \mathcal{N}(0, 1) \\
  x_i & \overset{i.i.d.}{=} \mathcal{N}(m, L L^T) \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

In =Turing.jl=
#+begin_src jupyter-julia :session jl
using Turing

L = [
    10 0;
    10 10
]

@model demo(x) = begin
    μ ~ MvNormal(zeros(2), ones(2))

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(μ, L * transpose(L))
    end
end
#+end_src

#+HTML: </div>

#+REVEAL: split

Generate =n = 100= samples with true mean =μ = [0.0, 0.0]=

#+begin_src jupyter-julia :session jl
# Data generation
n = 100
μ_true = 0.5 .* ones(2)  # <= different from original problem
likelihood = MvNormal(μ_true, L * transpose(L))
xs = rand(likelihood, n)
#+end_src

#+REVEAL: split

#+HTML: <div id="mvnormal-comparison">

#+CAPTION: True posterior
file:figures/mvnormal-1-posterior.png

#+CAPTION: MF-ADVI
file:figures/mvnormal-1-mfvi-elbo.png

#+HTML: <div class="fragment (appear)">

#+CAPTION: NF-ADVI (rational-quadratic)
file:figures/mvnormal-1-nfvi-elbo.png

#+CAPTION: NF-ADVI (affine)
file:figures/mvnormal-1-nfvi-elbo-affine.png

#+HTML: </div>

#+HTML: </div>

* Combine _EVERYTHING_

** Simple non-diagonal Gaussian (cont.)

#+begin_src jupyter-julia :session jl-nf-advi :exports none :results none
# SETUP
using Flux
using Flux: Optimise
using LinearAlgebra, Random
using StatsFuns: softplus

using ProgressMeter

using Plots, StatsPlots

using ForwardDiff
using Logging

using Pkg; Pkg.activate("~/.julia/dev/Bijectors")

using Bijectors
using Bijectors: Shift, Scale, CouplingLayer, PartitionMask

using Turing
using Turing: Variational
using Turing.Core: TuringDiagNormal

rng = Random.seed!(1)

# Variational inference stuff
Affine(W, b; dim::Type{Val{N}} = Val{1}) where {N} = Shift(b; dim = Val{N}) ∘ Scale(W; dim = Val{N})
g(θ) = Affine(exp(θ[1]) + 0.1, θ[2]; dim = Val{1})

function build_bijector(θ)
    σ = exp.(θ[1:2]) .+ 0.01
    μ = θ[3:4]

    W1 = reshape(θ[5:6], (2, 1))
    b1 = θ[7:8]
    W2 = reshape(θ[9:12], (2, 2))
    b2 = θ[13:14]

    nn1 = Chain(Dense(W1, b1, relu), Dense(W2, b2))

    W1 = reshape(θ[15:16], (2, 1))
    b1 = θ[17:18]
    W2 = reshape(θ[19:22], (2, 2))
    b2 = θ[23:24]

    nn2 = Chain(Dense(W1, b1, relu), Dense(W2, b2))

    mask1 = PartitionMask(2, [2], [1], nothing)
    cl1 = CouplingLayer(g, mask1, nn1)
    
    mask2 = PartitionMask(2, [1], [2], nothing)
    cl2 = CouplingLayer(g, mask2, nn2)

    return cl2 ∘ cl1 ∘ Affine(σ, μ)
end

# Posterior
L = [1. 0.; 1. 1.] .* 10
prior = MvNormal(zeros(2), ones(2))
likelihood = MvNormal(zeros(2), L * L')

# Data
n = 100
xs = rand(likelihood, n)

Σ = inv(inv(prior.Σ) + n * inv(likelihood.Σ))
μ = vec(Σ * (inv(prior.Σ) * prior.μ + n * inv(likelihood.Σ) * mean(xs; dims = 2)))
posterior = MvNormal(μ, Σ)

samples_posterior = rand(posterior, 1000)

# Turing model
@model gaussian(xs) = begin
    μ ~ prior

    for i = 1:size(xs, 2)
        xs[:, i] ~ MvNormal(μ, L * L')
    end
end

m = gaussian(xs)

# NF
BATCH_SIZE = 50
NUM_OPT_STEPS = 1_000

var_info = Turing.VarInfo(m)
function f(θ)
    b = build_bijector(θ)
    q = transformed(base, b)

    z₀, zₖ, logjac, logq = forward(q, BATCH_SIZE)
    lp = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:BATCH_SIZE]
    return - mean(lp + logjac) #ELBO
end

# initialization
base = Turing.Core.TuringDiagNormal(zeros(2), ones(2))

num_params = 34
θ = randn(num_params) ./ sqrt(num_params)

# optimize
opt = Variational.DecayedADAGrad(1e-3)

steps_taken = 0
prog = ProgressMeter.Progress(NUM_OPT_STEPS)

# initial ELBO
b = build_bijector(θ)
q = transformed(base, b)

z₀, zₖ, logjac, logq = forward(q, 100)
prev_logq = mean(logq)
prev_logjac = mean(logjac)

logp = logpdf(posterior, zₖ)
prev_logp = mean(logp)

logjoint = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:size(zₖ, 2)]
prev_logjoint = mean(logjoint)

prev_kl = mean(logq - logjoint)
prev_elbo = mean(logjoint + logjac) + entropy(q)


for i = 1:NUM_OPT_STEPS
    Δ = ForwardDiff.gradient(f, θ)

    ∇_norm = norm(Δ)
    Optimise.apply!(opt, θ, Δ)

    θ = θ .- Δ

    if i % 10 == 0
        b = build_bijector(θ)
        q = transformed(base, b)

        z₀, zₖ, logjac, logq = forward(q, 100)
        prev_logq = mean(logq)
        prev_logjac = mean(logjac)
        logp = logpdf(posterior, zₖ)
        prev_logp = mean(logp)
        
        prev_kl = mean(logq - logp)

        logjoint = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:100]
        prev_logjoint = mean(logjoint)
        prev_elbo = mean(logjoint + logjac) + entropy(q)

    end

    ProgressMeter.next!(
        prog;
        showvalues = [
            (:kl, prev_kl),
            (:elbo, prev_elbo),
            (:∇_norm, ∇_norm),
            (:prev_logjac, prev_logjac),
            (:prev_logp, prev_logp),
            (:prev_logjoint, prev_logjoint),
            (:prev_logq, prev_logq)
        ]
    )
end
θ_elbo = copy(θ)
b = build_bijector(θ_elbo)
q_nf_elbo = transformed(base, b)
samples_nf_elbo = rand(q_nf_elbo, 1000)

# visaulize
x_range = -2.0:0.1:1.5
y_range = -2.5:0.1:2.0

contour(x_range, y_range, (x, y) -> pdf(posterior, [x, y]), label = "Posterior")
contour!(x_range, y_range, (x, y) -> pdf(q_nf_elbo, [x, y]), label = "NF-ADVI")

# Okay, so now we use this as a prior
flow = q_nf_elbo;
#+end_src

#+begin_src jupyter-julia :session jl-nf-advi :exports none
flow # <= flow from earlier

L = [
    10. 0.;
    10. 10.
]
#+end_src

We change the true mean $\mu$ from =[0, 0]= to =[0.5, 0.5]=

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
# Data generation
n = 100
μ_true = 0.5 .* ones(2)  # <= different from original problem
likelihood = MvNormal(μ_true, L * transpose(L))
xs = rand(likelihood, n)
#+end_src

#+RESULTS:
: 2×100 Array{Float64,2}:
:  -15.3813  3.52882  -5.81578  -19.7243  …  -1.26671    8.8258   -5.49379
:  -12.4074  2.51113   0.42283  -26.9775     -0.724036  19.7033  -17.2421 

#+HTML: <div class="fragment (appear)">

And add the normalizing flow variational posterior from before as a /prior/

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
@model flow_demo(x) = begin
    μ ~ flow # <= NF-ADVI applied to `MvNormal` from before

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(μ, L * transpose(L))
    end
end
#+end_src 

#+HTML: </div>

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports none
prior = posterior # <= because we use previous posterior

Σ = inv(inv(prior.Σ) + n * inv(likelihood.Σ))
μ = vec(Σ * (inv(prior.Σ) * prior.μ + n * inv(likelihood.Σ) * mean(xs; dims = 2)))
posterior = MvNormal(μ, Σ)

samples_posterior = rand(posterior, 10_000)
#+end_src

#+RESULTS:
: 2×10000 Array{Float64,2}:
:  -0.15542  -0.174742   0.211021  …   0.550523  -0.623962   0.2692  
:  -1.64341  -0.205921  -0.686019     -1.4739    -1.64327   -0.174998

#+REVEAL: split

Then we perform exact inference using MCMC

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
# Sample
m = flow_demo(xs)
samples = sample(m, NUTS(20_000, 1000, 0.65))
#+end_src

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
┌ Info: Found initial step size
│   init_ϵ = 0.8
└ @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
┌ Info: Finished 1000 adapation steps
│   adaptor = StanHMCAdaptor(n_adapts=1000, pc=DiagPreconditioner, ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.65, state.ϵ=0.9161520150457593), init_buffer=75, term_buffer=50)
│   τ.integrator = Leapfrog(ϵ=0.916)
│   h.metric = DiagEuclideanMetric([0.256373, 0.441133])
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
┌ Info: Finished 20000 sampling steps in 115.067615846 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.256373, 0.441133]))
│   τ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(ϵ=0.916), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 23143.857593560853
│   average_acceptance_rate = 0.8397687531321688
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
#+END_EXAMPLE
#+BEGIN_EXAMPLE
Object of type Chains, with data of type 19000×12×1 Array{Union{Missing, Float64},3}

Log evidence      = 0.0
Iterations        = 1:19000
Thinning interval = 1
Chains            = 1
Samples per chain = 19000
internals         = acceptance_rate, eval_num, hamiltonian_energy, is_accept, log_density, lp, n_steps, numerical_error, step_size, tree_depth
parameters        = μ[1], μ[2]

2-element Array{ChainDataFrame,1}

Summary Statistics
. Omitted printing of 1 columns
│ Row │ parameters │ mean      │ std      │ naive_se   │ mcse       │ ess     │
│     │ [90mSymbol[39m     │ [90mFloat64[39m   │ [90mFloat64[39m  │ [90mFloat64[39m    │ [90mFloat64[39m    │ [90mAny[39m     │
├─────┼────────────┼───────────┼──────────┼────────────┼────────────┼─────────┤
│ 1   │ μ[1]       │ -0.240894 │ 0.526839 │ 0.00382209 │ 0.00519204 │ 11004.0 │
│ 2   │ μ[2]       │ -0.565251 │ 0.676478 │ 0.00490768 │ 0.00689196 │ 10801.6 │

Quantiles

│ Row │ parameters │ 2.5%     │ 25.0%     │ 50.0%     │ 75.0%     │ 97.5%    │
│     │ [90mSymbol[39m     │ [90mFloat64[39m  │ [90mFloat64[39m   │ [90mFloat64[39m   │ [90mFloat64[39m   │ [90mFloat64[39m  │
├─────┼────────────┼──────────┼───────────┼───────────┼───────────┼──────────┤
│ 1   │ μ[1]       │ -1.27603 │ -0.596539 │ -0.238923 │ 0.123116  │ 0.7855   │
│ 2   │ μ[2]       │ -1.91775 │ -1.0214   │ -0.558174 │ -0.107487 │ 0.743114 │
#+END_EXAMPLE
:END:

#+HTML: <div class="fragment (appear)">

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports results :file figures/flow-in-generative-model.svg
samples_vals = samples[:μ].value

x_range = -2.0:0.1:2.0
y_range = -2.5:0.1:2.0

histogram2d(samples_vals[:, 1], samples_vals[:, 2], bins = 25, normed = true, fill = cgrad(:viridis))
contour!(x_range, y_range, (x, y) -> pdf(posterior, [x, y]), label = "Posterior", linecolor = cgrad(:deep), linewidth = 3)
#+end_src

#+CAPTION: Results of using =NUTS= to sample from generative model with NF prior. Contour plot is of /true/ posterior, while histogram is of the =NUTS= samples.
#+RESULTS:
[[file:figures/flow-in-generative-model.svg]]

#+HTML: </div>

** Example: selling napkins like a professional
#+ATTR_REVEAL: :frag (appear)
- You own =k= ice-cream parlours on a beach
- Business is going well → want to expand & diversify
- _Genius idea #1:_ /on-the-move napkin salesmen/

#+REVEAL: split

#+ATTR_REVEAL: :frag (appear)
- Issue #1: what should the "napkin-route" be?
- You do have observations of which parlour people bought ice-cream at
  - E.g. $x_i = 1$ if the i-th customer bought ice cream at Parlour #1
- _Genius idea #2:_ Bayesian inference to get $p\big(\text{location} \mid \left\{ x_i \right\}_{i = 1}^n \big)$
  - Use posterior to decide where to define the napkin-route
  - Generative model is
    \begin{equation*}
    \begin{split}
      \mathrm{loc}_i &\sim p(\text{location}) \\
      \pi_i &:= f(\mathrm{loc}_i) \in \mathbb{R}^{k} \\
      x_i &\sim \mathrm{Categorical}(\pi_i), \quad i = 1, \dots, n
    \end{split}
    \end{equation*}
    where $f$ maps the location to some probability vector.
- Issue #2: need a prior for beach-people's locations
- _Genius idea #3:_ get hacker friend to hack everyone's phones to obtain location data, then perform density estimation on this location data to get a /prior/ over the locations.


#+REVEAL: split

#+CAPTION: Beach plot. =locations= (yellow) refer to samples from the location prior (density estimated using normalizing flow). The blue part represents the sea, and the black part represents land which is /not/ part of the beach so we don't care about it.
file:figures/ice-cream/beach_annotated.svg

#+REVEAL: split

#+CAPTION: Beach prior.
file:figures/ice-cream/prior_10000_1000.svg

*** Observations & model

Generate fake data (most people buy from Parlour #1)
#+begin_src jupyter-julia :session jl
fake_samples = [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1]
num_fake_samples = length(fake_samples)
#+end_src

#+HTML: <div class="fragment (appear)">

Define a =Model= which uses a NF as prior

#+begin_src jupyter-julia :session jl
@model napkin_model(x, ::Type{TV} = Vector{Float64}) where {TV} = begin
    locs = Vector{TV}(undef, length(x))
    
    for i ∈ eachindex(x)
        # We sample from the original distribution then transform to help NUTS.
        # Could equivalently have done `locs[i] ~ td` but more difficult to sample from.
        locs[i] ~ td.dist
        loc = td.transform(locs[i])

        # Compute a notion of "distance" from `loc` to the two different ice-cream parlours
        d1 = exp(- norm(parlour1 - loc))
        d2 = exp(- norm(parlour2 - loc))

        # The closer `loc` is to a ice-cream parlour, the more likely customer at `loc`
        # will buy from that instead of the other.
        πs = [d1 / (d1 + d2), d2 / (d1 + d2)]
        
        x[i] ~ Categorical(πs)
    end
end
#+end_src

#+HTML: </div>

#+begin_src jupyter-julia :session jl :exports none
# Example of πs
locs = rand(td, 10)
d1 = exp.(- [norm(parlour1 - locs[:, i]) for i = 1:size(locs, 2)])
d2 = exp.(- [norm(parlour2 - locs[:, i]) for i = 1:size(locs, 2)])
ds = hcat(d1, d2)
πs = ds ./ sum(ds; dims = 2)
#+end_src

#+REVEAL: split

Bayesian inference time

#+begin_src jupyter-julia :session jl
# Instantiate model
m = napkin_model(fake_samples)

# Sample using NUTS
num_mcmc_samples = 10_000
mcmc_warmup = 1_000
samples = sample(m, NUTS(num_mcmc_samples + mcmc_warmup, mcmc_warmup, 0.65));
#+end_src

#+begin_src jupyter-julia :session jl :exports none :results none
posterior_locs_samples = reshape(samples[:locs].value[:, :, 1], (:, num_fake_samples, 2))

x_samples = rand(td, 10_000)

# CHECK ONE OF THE CUSTOMERS

customer_idx = rand(1:num_fake_samples)
# Transform because samples will be untransformed
posterior_locs_idx = td.transform(posterior_locs_samples[:, customer_idx, :]')

posterior_locs_idx = td.transform(reshape(posterior_locs_samples, (:, 2))')

mean_locs = mean(posterior_locs_idx; dims = 2)
std_locs = std(posterior_locs_idx; dims = 2)

p1 = scatter(x_samples[1, :], x_samples[2, :], label = "locations", markerstrokewidth = 0, color = :orange, alpha = 0.3)

xlims!(-10.0, 30.0)
ylims!(-15.0, 15.0)

# scatter!(mean_locs[1:1], mean_locs[2:2], xerr = std_locs[1:1], yerr = std_locs[2:2], label = "")

histogram2d!(posterior_locs_idx[1, :], posterior_locs_idx[2, :], bins = 100, normed = true, alpha = 0.8, color = cgrad(:viridis))
title!("Posterior")

scatter!(parlour1[1:1], parlour1[2:2], label = "Parlour #1", color = :red, markersize = 5)
scatter!(parlour2[1:1], parlour2[2:2], label = "Parlour #2", color = :blue, markersize = 5)

p2 = scatter(x_samples[1, :], x_samples[2, :], label = "locations", markerstrokewidth = 0, color = :orange, alpha = 0.3)

xlims!(-10.0, 30.0)
ylims!(-15.0, 15.0)

histogram2d!(x_samples[1, :], x_samples[2, :]; bins = 100, normed = true, alpha = 0.8, color = cgrad(:viridis))
title!("Prior")

scatter!(parlour1[1:1], parlour1[2:2], label = "Parlour #1", color = :red, markersize = 5)
scatter!(parlour2[1:1], parlour2[2:2], label = "Parlour #2", color = :blue, markersize = 5)

plot(p1, p2, layout = (2, 1), size = (500, 1000))
#+end_src

#+HTML: <div class="fragment (appear">

#+CAPTION: Beach posterior.
file:figures/ice-cream/posterior_10000_1000.svg

#+HTML: </div>

#+REVEAL: split

file:figures/ice-cream/customer_10000_1000_1.svg

* Thank you!
#+HTML: <div><div><code>TuringLang</code> (website): <a href="https://turing.ml">https://turing.ml</a></div><div><code>TuringLang</code> (Github): <a href="https://github.com/TuringLang">https://github.com/TuringLang</a></div><div><code>Turing.jl</code> (Github): <a href="https://github.com/TuringLang/Turing.jl">https://github.com/TuringLang/Turing.jl</a></div><div><code>Bijectors.jl</code> (Github): <a href="https://github.com/TuringLang/Bijectors.jl">https://github.com/TuringLang/Bijectors.jl</a></div></div>
