#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t toc:nil


#+REVEAL_ROOT: file:///home/tor/Projects/mine/presentations/cambridge-julia-meetup/assets/reveal.js-3.8.0/
#+REVEAL_MATHJAX_URL: file:///home/tor/Projects/mine/presentations/cambridge-julia-meetup/assets/MathJax-2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+REVEAL_TITLE_SLIDE: <div><h1>You have data and I have distributions</h1><h3>A talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></h3><div style="margin: -200px auto; opacity: 0.2;"><p><object data="https://turing.ml/dev/assets/images/turing-logo-wide.svg"></object></p></div></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: moon

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: You have data and I have distributions: a talk on =Turing.jl= and =Bijectors.jl=

* Overview
#+ATTR_REVEAL: :frag (appear)
1. Bayesian inference
   - Why you want to do Bayesian inference
   - What it means to do Bayesian inference
2. =Turing.jl= on a simple example
   - Bayesian inference
   - /Approximate/ Bayesian inference (variational inference)
3. =Bijectors.jl=:
   - What it's about
   - Why it's neat
   - Normalising flow
4. Combining /everything/

* Bayesian inference and such
#+ATTR_REVEAL: :frag (appear)
- Have some dataset $\left\{ x_i \right\}_{i = 1}^n$
- Believe data $\left\{ x_i \right\}_{i = 1}^n$ was generated by some process and we're interested in inferring parameters $\theta$ of this process
  - E.g. in linear regression you have data $\left\{ (x_i, y_i) \right\}_{i = 1}^n$ and want to infer coefficients $\theta := \beta$

#+REVEAL: split

_Frequentist_
#+begin_quote
A single point as the estimate of $\theta$ is good enough.
#+end_quote

#+HTML: <div class="fragment (appear)">

_Bayesian_ 
#+begin_quote
But we have _finite_ data! And this dataset just happen to give you the one and only $\theta$?!

No, no, no, we need a distribution over the $\theta$ given the data (a *posterior*):
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big)
\end{equation*}
#+end_quote

#+HTML: </div>

** Bayes' rule
*Bayes' rule* gives us
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big) = \frac{p\big(\left\{ x_i \right\}_{i = 1}^n \mid \theta\big) p(\theta)}{p\big(\left\{ x_i \right\}_{i = 1}^n\big)}
\end{equation*}
#+HTML: <div class="fragment (appear)">
or, since the denominator is constant,
\begin{equation*}
\begin{split}
  p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big) &\propto p \big( \left\{ x_i \right\}_{i = 1}^n \mid \theta \big) p(\theta) \\
  &= p \big( \theta, \left\{ x_i \right\}_{i = 1}^n \big)
\end{split}
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
For the family of inference methods known as /Markov Chain Monte-Carlo (MCMC)/, this proportional factor is all we need.
#+HTML: </div>

* Setup
#+begin_src jupyter-julia :session jl :results silent :exports code :async yes
using Pkg; Pkg.activate(".")
#+end_src

#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Plots, StatsPlots
#+end_src

#+begin_src jupyter-julia :session jl :exports code :async yes :results silent
using Bijectors
#+end_src

#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Turing
#+end_src

_Disclaimer:_ All functionality in this talk is not yet available on the master branch of =Bijectors.jl=, but should be soon‚Ñ¢.

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports both
versioninfo()
#+end_src

#+RESULTS:
: Julia Version 1.1.1
: Commit 55e36cc308 (2019-05-16 04:10 UTC)
: Platform Info:
:   OS: Linux (x86_64-pc-linux-gnu)
:   CPU: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
:   WORD_SIZE: 64
:   LIBM: libopenlibm
:   LLVM: libLLVM-6.0.1 (ORCJIT, skylake)

* =Turing.jl=
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
=Turing.jl= is a (universal) _probabilistic programming language_ (PPL) in Julia.
#+end_quote

#+ATTR_REVEAL: :frag (appear)
What does that even mean?

#+ATTR_REVEAL: :frag (appear)
1. Specify generative model in Julia with neat syntax
2. Bayesian inference to estimate posterior $p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$ using a vast library of MCMC samplers
3. ???
4. Profit (in expectation)!!!

** Example: Gaussian-InverseGamma conjugate model
#+ATTR_REVEAL: :frag (appear)
In mathematical notation:
#+ATTR_REVEAL: :frag (appear)
\begin{equation*}
\begin{split}
  s & \sim \mathrm{InverseGamma}(2, 3) \\
  m & \sim \mathcal{N}(0, \sqrt{s}) \\
  x_i & \sim \mathcal{N}(m, \sqrt{s}), \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+ATTR_REVEAL: :frag (appear)
In =Turing.jl=:
#+ATTR_REVEAL: :frag (appear)
#+begin_src jupyter-julia :session jl :exports code
@model model(x) = begin
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, ‚àös)
    for i = 1:length(x)
        x[i] ~ Normal(m, ‚àös)
    end
end
#+end_src

#+RESULTS:
: model (generic function with 2 methods)

#+REVEAL: split

Generate some fake data and instantiate the model

#+begin_src jupyter-julia :session jl :exports code
xs = randn(1_000)
m = model(xs)
#+end_src

#+HTML: <div class="fragment (appear)">

Now sample to obtain posterior $p\big(m, s \mid \left\{ x_i \right\}_{i = 1}^n \big)$

#+begin_src jupyter-julia :session jl :exports code
# Sample 1000 samples using HMC
samples_nuts = sample(m, NUTS(10_000, 200, 0.65));
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
‚îå Info: Found initial step size
‚îÇ   init_œµ = 0.025
‚îî @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
‚îå Info: Finished 200 adapation steps
‚îÇ   adaptor = StanHMCAdaptor(n_adapts=200, pc=DiagPreconditioner, ssa=NesterovDualAveraging(Œ≥=0.05, t_0=10.0, Œ∫=0.75, Œ¥=0.65, state.œµ=1.302609925324549), init_buffer=75, term_buffer=50)
‚îÇ   œÑ.integrator = Leapfrog(œµ=1.3)
‚îÇ   h.metric = DiagEuclideanMetric([0.00132985, 0.000834938])
‚îî @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
‚îå Info: Finished 10000 sampling steps in 1.502001043 (s)
‚îÇ   h = Hamiltonian(metric=DiagEuclideanMetric([0.00132985, 0.000834938]))
‚îÇ   œÑ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(œµ=1.3), max_depth=10), Œî_max=1000.0)
‚îÇ   EBFMI_est = 2115.310793041227
‚îÇ   average_acceptance_rate = 0.8436075614790801
‚îî @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
#+END_EXAMPLE

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Aaaand we can plot the resulting (empirical) posterior

#+begin_src jupyter-julia :session jl :exports code :file figures/gaussian-inversegamma-hmc.svg
plot(samples_nuts[[:s, :m]])
#+end_src

#+HTML: </div>

#+REVEAL: split

[[file:figures/gaussian-inversegamma-hmc.svg]]

** *Approximate* inference (Variational inference)
Might be happy with an /approximation/ to your posterior $p \big( \theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$.

#+HTML: <div class="fragment (appear)">

*Variational inference (VI)* is an approximate approach which formulates the problem as an /optimization/ problem:
#+HTML: </div>
#+HTML: <div class="fragment (appear)">
\begin{equation*}
\underset{q \in \mathscr{Q}}{\text{argmin}}\ \mathrm{D_{KL}} \big( q(\theta) \mid p(\theta \mid \left\{ x_i \right\}_{i = 1}^n ) \big) \quad \text{or} \quad \underset{q \in \mathscr{Q}}{\text{argmax}}\ \mathrm{ELBO} \big( q(\theta) \big)
\end{equation*}
where
\begin{equation*}
\mathrm{ELBO} \big( q(\theta) \big) = \mathbb{E}_{\theta \sim q(\theta)} \big[ \log p\big(\theta, \left\{ x_i \right\}_{i = 1}^n \big) \big] + \mathbb{H} \big( q(\theta) \big)
\end{equation*}

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
_Caveat:_ usually assume $\mathscr{Q}$ is the family of Gaussians with /diagonal/ covariance.

*** Automatic Differentiation Variational Inference (ADVI)
(Mean-field) ADVI is a simple but flexible VI approach that exists in =Turing.jl=
#+begin_src jupyter-julia :session jl :results none :exports code
# "Configuration" for ADVI
# - 10 samples for gradient estimation
# - Perform 15 000 optimization steps
advi = ADVI(10, 15_000)

# Perform `ADVI` on model `m` to get variational posterior `q`
q = vi(m, advi)
#+end_src

#+HTML: <div class="fragment (appear)">
To sample and compute probabilities
#+begin_src jupyter-julia :session jl :exports code
xs = rand(q, 10)
logpdf(q, xs)
#+end_src
#+HTML: </div>

#+REVEAL: split

#+CAPTION: =ADVI= applied to the Normal-InverseGamma generative model from earlier. _Disclaimer:_ this plot is generated by writing the optimization loop myself rather than using the simple =vi(m, advi)= call. See =test/skipped/advi_demo.jl= in =Turing.jl= for the code used.
[[./figures/advi_w_elbo_fps15_125_forward_diff.gif]]

** Benchmarks of =HMC=
file:figures/turing-benchmarks.png

* =Bijectors.jl=
#+ATTR_REVEAL: :frag (appear)
#+name: def:bijector
#+begin_definition :title ""
A *bijector* or *diffeomorphism* is a differentiable /bijection/ $b$ with a /differentiable/ inverse $b^{-1}$.
#+end_definition

#+ATTR_REVEAL: :frag (appear)
For example $b(x) = \exp(x)$
#+ATTR_REVEAL: :frag (appear)
- $\exp$ is differentiable
- $\exp$ has inverse $\log$
- $\log$ is differentiable (on $(0, \infty)$)
#+ATTR_REVEAL: :frag (appear)
So $\exp$ (and $\log$) is a bijector!

#+REVEAL: split

In =Bijectors.jl=

#+begin_src jupyter-julia :session jl :exports both :results none
using Bijectors; using Bijectors: Exp, Log

b = Exp()
b‚Åª¬π = inv(b)

b‚Åª¬π isa Log
#+end_src

#+RESULTS:
: true

#+HTML: <div class="fragment (appear)">
We can evaluate a =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
x = 0.0
b(x) == 1.0  # since e‚Å∞ = 1
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We can /compose/ bijectors to get a new =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
(b ‚àò b) isa Bijector
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

And evaluate compositions of bijectors

#+begin_src jupyter-julia :session jl :exports both
(b‚Åª¬π ‚àò b)(x) == x
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
What about more complex/deeper compositions?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
cb = b ‚àò b ‚àò b
cb‚Åª¬π = inv(cb)        # <= inversion of a "large" composition

(cb‚Åª¬π ‚àò cb)(x) == x
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We'll see later that of particular interest is the term

\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Which works seamlessly even for compositions

#+begin_src jupyter-julia :session jl :exports both
logabsdetjac(cb, x)
#+end_src

#+RESULTS:
: 3.718281828459045

#+HTML: </div>

** How does it relate to distributions?
#+HTML: <div class="fragment (appear)">
Consider
\begin{equation*}
x \sim \mathcal{N}(1, 5)
\end{equation*}
#+HTML: </div>
#+HTML: <div class="fragment (appear)">
Or, equivalently,
\begin{equation*}
\begin{split}
  \xi &\sim \mathcal{N}(0, 1) \\
  x &:= 1 + 5 \xi
\end{split}
\end{equation*}
#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports none :results none
using Random; Random.seed!(42)
#+end_src

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-pdfs.svg :results none
d1 = Normal(1.0, 5.0) # ùí©(1, 5)
xs = rand(d1, 100_000)

d2 = Normal(0.0, 1.0) # ùí©(0, 1)
Œæ = rand(d2, 100_000)
ys = 1.0 .+ Œæ .* 5.0  # y ~ ùí©(1, 5)

density(xs, label = "d1", linewidth = 3)
density!(ys, label = "d2", linewidth = 3)
#+end_src

[[file:figures/normal-pdfs.svg]]

#+REVEAL: split

=Bijector= + =Distribution= = another =Distribution=

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both :exports code
using Bijectors: Shift, Scale

# Define the transform
b = Shift(1.0) ‚àò Scale(5.0)           # => x ‚Ü¶ 1.0 + 5.0x
td = transformed(Normal(0.0, 1.0), b) # => ùí©(1.0, 5.0)
#+end_src

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Moreover, =td= is a =TransformedDistribution= /and/

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
td isa Distribution
#+end_src

#+RESULTS:
: true

Yay!

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src jupyter-julia :session jl :exports both
y = rand(td)
# Ensure we have the same densities
logpdf(td, y) ‚âà logpdf(d1, y)
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-transformed-pdfs.svg :results none
x_range = -14.0:0.05:16.0
plot(x_range, x -> pdf(Normal(1.0, 5.0), x), label = "d1", linewidth = 4, linestyle = :dash)
plot!(x_range, x -> pdf(td, x), label = "td", linewidth = 4, alpha = 0.6)
#+end_src

#+CAPTION: Density of $\mathcal{N}(0, 1)$ transformed by $\xi \mapsto 1 + 5\xi$ (labeled =td=) and $\mathcal{N}(1, 5)$ (labeled =d1=).
[[file:figures/normal-transformed-pdfs.svg]]

#+REVEAL: split

#+begin_quote
Great; just another way to draw samples from a normal distribution...
#+end_quote

#+ATTR_REVEAL: :frag (appear)
Or /is it/?

#+ATTR_REVEAL: :frag (appear)
#+begin_src jupyter-julia :session jl :exports code :file figures/simple-nf-pdf.svg :results none
using Random; Random.seed!(10); # <= chosen because of nice visualization
d = MvNormal(zeros(1), ones(1))

# "Deep" transformation
b = (
    RadialLayer(1) ‚àò RadialLayer(1) ‚àò
    RadialLayer(1) ‚àò RadialLayer(1)
)
td = transformed(d, b)

# Sample from flow
xs = rand(td, 10_000)

x_range = minimum(xs):0.05:maximum(xs)
lps = pdf(td, reshape(x_range, (1, :)))  # compute the probabilities

histogram(vec(xs), bins = 100; normed = true, label = "", alpha = 0.7)
xlabel!("y")
plot!(x_range, lps, linewidth = 3, label = "p(y)")
#+end_src

#+REVEAL: split
#+LaTeX: \clearpage

#+RESULTS:
[[file:figures/simple-nf-pdf.svg]]

That doesn't look very /normal/, now does it?!

*** How?
It turns out that if $b$ is a =Bijector=, the process
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}
/induces/ a density $\tilde{p}(y)$ defined by
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}

#+HTML: <div class="fragment (appear)">
Therefore if we can compute $\mathcal{J}_{b^{-1}}(y)$ we can indeed compute $\tilde{p}(y)$!
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
But is this actually useful?
#+end_quote

** Normalising flows: parameterising $b$
One might consider constructing a /parameterised/ =Bijector= $b_{\phi}$.

#+HTML: <div class="fragment (appear)">
Given a density $p(x)$ we can obtain a parameterised density
\begin{equation*}
\tilde{p}_{\phi}(y) = p \big( b_{\phi}^{-1}(y) \big) \left| \det \mathcal{J}_{b_{\phi}^{-1}}(y) \right|
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
$b_{\phi}$ is often referred to as a *normalising flow (NF)*
#+end_quote

#+HTML: <div class="fragment (appear)">
Can now optimise any objective over distributions, e.g. perform /maximum likelihood estimation/ (MLE) for some given i.i.d. dataset $\left\{ y_i \right\}_{i = 1}^n$
\begin{equation*}
\underset{\phi}{\text{argmax}}\ \sum_{i = 1}^{n} \tilde{p}_{\phi}(y_i)
\end{equation*}
#+HTML: </div>

*** Example: MLE using NF
Consider an =Affine= transformation, i.e.
\begin{equation*}
\mathrm{aff}(x) = W x + b
\end{equation*}
for matrix $W$ and vector $b$,
#+HTML: <div class="fragment (appear)">
and a non-linear (but /invertible/) activation function, e.g. =LeakyReLU=
\begin{equation*}
a(x) = 
\begin{cases}
  x & \text{if } x \ge 0 \\
  \alpha x & \text{if } x < 0
\end{cases}
\end{equation*}
for some /non-zero/ $\alpha \in \mathbb{R}$ (usually chosen to be very small).
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Can define a "deep" NF by composing such transformations! Looks familiar?

#+HTML: <div class="fragment (appear)">
Yup; it's basically an invertible neural network! (assuming $\det W \ne 0$)

#+begin_src jupyter-julia :session jl
layers = [LeakyReLU(Œ±[i]) ‚àò Affine(W[i], b[i]) for i = 1:num_layers]

b = foldl(‚àò, layers)
td = transformed(base_dist, b)  # <= "deep" normalising flow!
#+end_src

#+HTML: </div>

#+REVEAL: split

#+ATTR_HTML: :width 35%
#+CAPTION: Empirical density estimate (blue) compared with single batch of samples (red). Code can be found in =scripts/nf_banana.jl=.
file:figures/nf-banana-density-estimation.gif

** Methods

|-------------------------------------------------------------+-------------------------+--------------|
| Operation                                                   | Method                  | Automatic    |
|-------------------------------------------------------------+-------------------------+--------------|
| $b \mapsto b^{-1}$                                          | =inv(b)=                | $\checkmark$ |
| $(b_1, b_2) \mapsto (b_1 \circ b_2)$                        | =b1 ‚àò b2=               | $\checkmark$ |
| $(b_1, b_2) \mapsto [b_1, b_2]$                             | =stack(b1, b2)=         | $\checkmark$ |
| $(b, n) \mapsto b^n := b \circ \cdots \circ b$ (n times)    | =b^n=                   | $\checkmark$ |
|-------------------------------------------------------------+-------------------------+--------------|

#+REVEAL: split

|----------------------------------------------------------------+----------------------+--------------|
| Operation                                                      | Method               | Automatic    |
|----------------------------------------------------------------+----------------------+--------------|
| $x \mapsto b(x)$                                               | =b(x)=               | $\times$     |
| $y \mapsto b^{-1}(y)$                                          | =inv(b)(y)=          | $\times$     |
| $x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert$         | =logabsdetjac(b, x)= | AD           |
| $x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)$ | =forward(b, x)=      | $\checkmark$ |
|----------------------------------------------------------------+----------------------+--------------|

#+REVEAL: split

|--------------------------------------------------------------------+-------------------------+--------------|
| Operation                                                          | Method                  | Automatic    |
|--------------------------------------------------------------------+-------------------------+--------------|
| $p \mapsto q:= b_* p$                                              | =q = transformed(p, b)= | $\checkmark$ |
| $y \sim q$                                                         | =y = rand(q)=           | $\checkmark$ |
| $p \mapsto b$ s.t. $\mathrm{support}(b_* p) = \mathbb{R}^d$        | =bijector(p)=           | $\checkmark$ |
| $\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)$ | =forward(q)=            | $\checkmark$ |
|--------------------------------------------------------------------+-------------------------+--------------|

** Implementing a =Bijector=
#+begin_src jupyter-julia :session jl :eval no :exports code
using StatsFuns: logit, logistic

struct Logit{T<:Real} <: Bijector{0} # <= 0-dimensional, i.e. expects `Real` input (or `Vector` which is treated as batch)
    a::T
    b::T
end

(b::Logit)(x) = @. logit((x - b.a) / (b.b - b.a))
# `orig` contains the `Bijector` which was inverted
(ib::Inversed{<:Logit})(y) = @. (ib.orig.b - ib.orig.a) * logistic(y) + ib.orig.a

logabsdetjac(b::Logit, x) = @. - log((x - b.a) * (b.b - x) / (b.b - b.a))
#+end_src

#+REVEAL: split

#+begin_src jupyter-julia :session jl :eval no
julia> b = Logit(0.0, 1.0)
Logit{Float64}(0.0, 1.0)

julia> y = b(0.6)
0.4054651081081642

julia> inv(b)(y)
0.6

julia> logabsdetjac(b, 0.6)
1.4271163556401458

julia> logabsdetjac(inv(b), y) # defaults to `- logabsdetjac(b, inv(b)(x))`
-1.4271163556401458

julia> forward(b, 0.6)         # defaults to `(rv=b(x), logabsdetjac=logabsdetjac(b, x))`
(rv = 0.4054651081081642, logabsdetjac = 1.4271163556401458)
#+end_src

* NF-ADVI vs. MF-ADVI
#+HTML: <div class="fragment (appear)">

Consider $L = \begin{pmatrix} 10 & 0 \\ 10 & 10 \end{pmatrix}$ and
\begin{equation*}
\begin{split}
  m & \sim \mathcal{N}(0, 1) \\
  x_i & \overset{i.i.d.}{=} \mathcal{N}(m, L L^T) \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

In =Turing.jl=
#+begin_src jupyter-julia :session jl
using Turing

L = [
    10 0;
    10 10
]

@model demo(x) = begin
    Œº ~ MvNormal(zeros(2), ones(2))

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(Œº, L * transpose(L))
    end
end
#+end_src

#+HTML: </div>

#+REVEAL: split

Generate =n = 100= samples with true mean =Œº = [0.0, 0.0]=

#+begin_src jupyter-julia :session jl
# Data generation
n = 100
Œº_true = 0.5 .* ones(2)  # <= different from original problem
likelihood = MvNormal(Œº_true, L * transpose(L))
xs = rand(likelihood, n)
#+end_src

#+REVEAL: split

#+HTML: <div id="mvnormal-comparison">

#+CAPTION: True posterior
file:figures/mvnormal-1-posterior.png

#+CAPTION: MF-ADVI
file:figures/mvnormal-1-mfvi-elbo.png

#+HTML: <div class="fragment (appear)">

#+CAPTION: NF-ADVI (rational-quadratic)
file:figures/mvnormal-1-nfvi-elbo.png

#+CAPTION: NF-ADVI (affine)
file:figures/mvnormal-1-nfvi-elbo-affine.png

#+HTML: </div>

#+HTML: </div>

* Combining _EVERYTHING_

** Real-world example: selling napkins like a professional
#+ATTR_REVEAL: :frag (appear)
- You own =k= ice-cream parlours on a beach
- Business is going well ‚Üí want to expand & diversify
- _Genius idea #1:_ /on-the-move napkin salesmen/

#+REVEAL: split

#+ATTR_REVEAL: :frag (appear)
- Issue #1: what should the "napkin-route" be?
- You do have observations of which parlour people bought ice-cream at
  - E.g. $x_i = 1$ if the i-th customer bought ice cream at Parlour #1
- _Genius idea #2:_ Bayesian inference to get $p\big(\text{location} \mid \left\{ x_i \right\}_{i = 1}^n \big)$
  - Use posterior to decide where to define the napkin-route
  - Generative model is
    \begin{equation*}
    \begin{split}
      \mathrm{loc}_i &\sim p(\text{location}) \\
      \pi_i &:= f(\mathrm{loc}_i) \in \mathbb{R}^{k} \\
      x_i &\sim \mathrm{Categorical}(\pi_i), \quad i = 1, \dots, n
    \end{split}
    \end{equation*}
    where $f$ maps the location to some probability vector.
- Issue #2: need a prior for beach-people's locations
- _Genius idea #3:_ get hacker friend to hack everyone's phones to obtain location data, then perform density estimation on this location data to get a /prior/ over the locations.


#+REVEAL: split

#+CAPTION: Beach plot. =locations= (yellow) refer to samples from the location prior (density estimated using normalising flow). The blue part represents the sea, and the black part represents land which is /not/ part of the beach so we don't care about it.
file:figures/ice-cream/beach_annotated.svg

#+REVEAL: split

#+begin_src jupyter-julia :session jl :results none :exports none
include("scripts/nf_banana_improved.jl")
#+end_src

#+CAPTION: Beach prior. 6-layer NF with =Affine= and =LeakyReLU= as seen in animation from before.
file:figures/ice-cream/prior_10000_1000.svg


*** Observations & model

Generate fake data (most people buy from Parlour #1)
#+begin_src jupyter-julia :session jl
fake_samples = [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1]
num_fake_samples = length(fake_samples)
#+end_src

#+HTML: <div class="fragment (appear)">

Define a =Model= which uses a NF as prior

#+begin_src jupyter-julia :session jl
@model napkin_model(x, ::Type{TV} = Vector{Float64}) where {TV} = begin
    locs = Vector{TV}(undef, length(x))
    
    for i ‚àà eachindex(x)
        # We sample from the original distribution then transform to help NUTS.
        # Could equivalently have done `locs[i] ~ td` but more difficult to sample from.
        locs[i] ~ td.dist
        loc = td.transform(locs[i])

        # Compute a notion of "distance" from `loc` to the two different ice-cream parlours
        d1 = exp(- norm(parlour1 - loc))
        d2 = exp(- norm(parlour2 - loc))

        # The closer `loc` is to a ice-cream parlour, the more likely customer at `loc`
        # will buy from that instead of the other.
        œÄs = [d1 / (d1 + d2), d2 / (d1 + d2)]
        
        x[i] ~ Categorical(œÄs)
    end
end
#+end_src

#+HTML: </div>

#+begin_src jupyter-julia :session jl :exports none
# Example of œÄs
locs = rand(td, 10)
d1 = exp.(- [norm(parlour1 - locs[:, i]) for i = 1:size(locs, 2)])
d2 = exp.(- [norm(parlour2 - locs[:, i]) for i = 1:size(locs, 2)])
ds = hcat(d1, d2)
œÄs = ds ./ sum(ds; dims = 2)
#+end_src

#+REVEAL: split

Bayesian inference time

#+begin_src jupyter-julia :session jl
# Instantiate model
m = napkin_model(fake_samples)

# Sample using NUTS
num_mcmc_samples = 10_000
mcmc_warmup = 1_000
samples = sample(m, NUTS(mcmc_warmup, 0.65), num_mcmc_samples);
#+end_src

#+begin_src jupyter-julia :session jl :exports none :results none
posterior_locs_samples = reshape(samples[:locs].value[:, :, 1], (:, num_fake_samples, 2))

x_samples = rand(td, 10_000)

# CHECK ONE OF THE CUSTOMERS

customer_idx = rand(1:num_fake_samples)
# Transform because samples will be untransformed
posterior_locs_idx = td.transform(posterior_locs_samples[:, customer_idx, :]')

posterior_locs_idx = td.transform(reshape(posterior_locs_samples, (:, 2))')

mean_locs = mean(posterior_locs_idx; dims = 2)
std_locs = std(posterior_locs_idx; dims = 2)

p1 = scatter(x_samples[1, :], x_samples[2, :], label = "locations", markerstrokewidth = 0, color = :orange, alpha = 0.3)

xlims!(-10.0, 30.0)
ylims!(-15.0, 15.0)

# scatter!(mean_locs[1:1], mean_locs[2:2], xerr = std_locs[1:1], yerr = std_locs[2:2], label = "")

histogram2d!(posterior_locs_idx[1, :], posterior_locs_idx[2, :], bins = 100, normed = true, alpha = 0.8, color = cgrad(:viridis))
title!("Posterior")

scatter!(parlour1[1:1], parlour1[2:2], label = "Parlour #1", color = :red, markersize = 5)
scatter!(parlour2[1:1], parlour2[2:2], label = "Parlour #2", color = :blue, markersize = 5)

p2 = scatter(x_samples[1, :], x_samples[2, :], label = "locations", markerstrokewidth = 0, color = :orange, alpha = 0.3)

xlims!(-10.0, 30.0)
ylims!(-15.0, 15.0)

histogram2d!(x_samples[1, :], x_samples[2, :]; bins = 100, normed = true, alpha = 0.8, color = cgrad(:viridis))
title!("Prior")

scatter!(parlour1[1:1], parlour1[2:2], label = "Parlour #1", color = :red, markersize = 5)
scatter!(parlour2[1:1], parlour2[2:2], label = "Parlour #2", color = :blue, markersize = 5)

plot(p1, p2, layout = (2, 1), size = (500, 1000))
#+end_src

#+HTML: <div class="fragment (appear">

#+CAPTION: Beach posterior.
file:figures/ice-cream/posterior_10000_1000.svg

#+HTML: </div>

#+REVEAL: split

Guaranteed great success. 

A couple of years later you're probably already in the business of bribing politicians to reduce worker-benefits for napkin-salesmen.

* Thank you
#+HTML: <div><div><code>TuringLang</code> (website): <a href="https://turing.ml">https://turing.ml</a></div><div><code>TuringLang</code> (Github): <a href="https://github.com/TuringLang">https://github.com/TuringLang</a></div><div><code>Turing.jl</code> (Github): <a href="https://github.com/TuringLang/Turing.jl">https://github.com/TuringLang/Turing.jl</a></div><div><code>Bijectors.jl</code> (Github): <a href="https://github.com/TuringLang/Bijectors.jl">https://github.com/TuringLang/Bijectors.jl</a></div></div>
