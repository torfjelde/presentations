#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_TITLE_SLIDE: <div><h1>You have data and I have distributions</h1><h3>A talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></h3></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: moon

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: You have data and I have distributions: a talk on =Turing.jl= and =Bijectors.jl=

* Overview
1. Bayesian inference and a bit of density estimation
   - Why you want to do Bayesian inference
   - What it means to do Bayesian inference
   - Brief mention of what "density estimation" is
2. =Turing.jl= on a simple example
   - Bayesian inference
   - /Approximate/ Bayesian inference (variational inference)
3. =Bijectors.jl=:
   - What it's about
   - Why it's neat
   - Normalizing flow
4. Normalizing flow ADVI
5. Combining /everything/

* Bayesian inference and such
- Have some dataset $\left\{ x_i \right\}_{i = 1}^n$
- Generally two problems which arise
  1. Have a expectation of some function $f_{\theta}(x)$ for which we want to infer the parameters $\theta$ given the data, e.g. linear regression
  2. We believe the data $\left\{ x_i \right\}_{i = 1}^n$ was generated by some process and we're interested in inferring parameters of this process, e.g. linear regression
- They're really both the same

#+REVEAL: split

_Frequentist_
#+begin_quote
A single point as estimate of $\theta^*$ is good enough.
#+end_quote

#+HTML: <div class="fragment (appear)">

_Bayesian_ 
#+begin_quote
With finite data you just happen to find the best $\theta^*$?! 

No, no, no, we need a *distribution* over the optimal $\theta^*$ given the data:
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big)
\end{equation*}
#+end_quote

#+HTML: </div>

** Motivation
#+begin_quote
But Tor, why do I even need _distributions_ for my data?
#+end_quote

Usually you have some 

#+HTML: <div class="fragment (highlight-red)">
*Goal:* find $\theta$ which best fits our observations $\left\{ x_i \right\}_{i = 1}^n$
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
*Goal*: find $p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$ which maximses the probability of our observations $\left\{ x_i \right\}_{i = 1}^n$


** Bayes' rule
*Bayes' rule* gives us
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big) = \frac{p\big(\left\{ x_i \right\}_{i = 1}^n \mid \theta\big) p(\theta)}{p\big(\left\{ x_i \right\}_{i = 1}^n\big)}
\end{equation*}
#+HTML: <div class="fragment (appear)">
or, since the denominator is constant,
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big) \propto p \big( \left\{ x_i \right\}_{i = 1}^n \mid \theta \big) p(\theta)
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
For the family of inference methods known as /Markov Chain Monte-Carlo (MCMC)/, this proportional factor is all we need.
#+HTML: </div>

** Reasons
- "But Tor, why do I need distributions for my data? Why is it not enough with just a /single point/ for my predictions?"
- Well then I'll ask you; do you want to know how certain you are about your 


* Setup
#+begin_src jupyter-julia :session jl :exports code :results silent
using Plots, StatsPlots
#+end_src

#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Turing
#+end_src


* =Turing.jl=
#+begin_quote
=Turing.jl= is a (universal) _probabilistic programming language_ (PPL) in Julia.
#+end_quote

#+ATTR_REVEAL: :frag (appear)
What does that even mean?

#+ATTR_REVEAL: :frag (appear)
1. Specify generative model in Julia with neat syntax
2. Perform Bayesian inference over the latent variables using a vast library of MCMC samplers
3. ???
4. Profit!!!

** Gaussian-InverseGamma conjugate model
#+ATTR_REVEAL: :frag (appear)
In mathematical notation:
#+ATTR_REVEAL: :frag (appear)
\begin{equation*}
\begin{split}
  s & \sim \mathrm{InverseGamma}(2, 3) \\
  m & \sim \mathcal{N}(0, \sqrt{s}) \\
  x_i & \sim \mathcal{N}(m, \sqrt{s}), \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+ATTR_REVEAL: :frag (appear)
In =Turing.jl=:
#+ATTR_REVEAL: :frag (appear)
#+begin_src jupyter-julia :session jl :exports code
@model model(x) = begin
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, √s)
    for i = 1:length(x)
        x[i] ~ Normal(m, √s)
    end
end
#+end_src

#+RESULTS:
: model (generic function with 2 methods)

** Exact inference: No U-turn Sampler (NUTS)
=NUTS= is a MCMC methods which generally works well with minimal tuning

#+begin_src jupyter-julia :session jl :exports code
xs = randn(1_000)
m = model(xs)

# Sample 1000 samples using HMC
samples_nuts = sample(m, NUTS(10_000, 200, 0.65));
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
┌ Info: Found initial step size
│   init_ϵ = 0.025
└ @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
┌ Info: Finished 200 adapation steps
│   adaptor = StanHMCAdaptor(n_adapts=200, pc=DiagPreconditioner, ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.65, state.ϵ=1.302609925324549), init_buffer=75, term_buffer=50)
│   τ.integrator = Leapfrog(ϵ=1.3)
│   h.metric = DiagEuclideanMetric([0.00132985, 0.000834938])
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
┌ Info: Finished 10000 sampling steps in 1.502001043 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.00132985, 0.000834938]))
│   τ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(ϵ=1.3), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 2115.310793041227
│   average_acceptance_rate = 0.8436075614790801
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
#+END_EXAMPLE

Aaaand we can plot the resulting (empirical) posterior

#+begin_src jupyter-julia :session jl :exports code :file figures/gaussian-inversegamma-hmc.svg
plot(samples_nuts[[:s, :m]])
#+end_src

#+REVEAL: split

[[file:figures/gaussian-inversegamma-hmc.svg]]

** *Approximate* inference (Variational inference)
Sometimes you might be happy with an /approximation/ to your posterior $p \big( \theta \mid \left\{ x_i \right\}_{i = 1}^n \big)$.

#+HTML: <div class="fragment (highlight-red)">

*Variational inference (VI)* is an approximate approach which formulates the problem as an /optimization/ problem:
\begin{equation*}
\underset{q}{\text{argmax}}\ \mathrm{ELBO} \big( q(\theta) \big) \quad \text{or} \quad \underset{q}{\text{argmin}}\ \mathrm{D_{KL}} \big( q(\theta) \mid p(\theta \mid \left\{ x_i \right\}_{i = 1}^n ) \big)
\end{equation*}
where
\begin{equation*}
\mathrm{ELBO} \big( q(\theta) \big) = \mathbb{E}_{\theta \sim q(\theta)} \big[ \log p\big(\theta, \left\{ x_i \right\}_{i = 1}^n \big) \big] + \mathbb{H} \big( q(\theta) \big)
\end{equation*}

#+HTML: </div>

*** Automatic Differentiation Variational Inference (ADVI)
ADVI is a simple but flexible VI approach that exists in =Turing.jl=
#+begin_src jupyter-julia :session jl :results none :exports code
# "Configuration" for ADVI
# - 10 samples for gradient estimation
# - Perform 15 000 optimization steps
advi = ADVI(10, 15_000)

# Perform `ADVI` on model `m` to get variational posterior `q`
q = vi(m, advi)
#+end_src

To sample and compute probabilities
#+begin_src jupyter-julia :session jl :exports code
xs = rand(q, 10)
logpdf(q, xs)
#+end_src


#+REVEAL: split

#+CAPTION: =ADVI= applied to the Normal-InverseGamma generative model from earlier. _Disclaimer:_ this plot is generated by writing the optimization loop myself rather than using the simple =vi(m, advi)= call.
[[./figures/advi_w_elbo_fps15_125_forward_diff.gif]]

** Benchmarks of =HMC=
file:figures/turing-benchmarks.png

* =Bijectors.jl=
#+ATTR_REVEAL: :frag (appear)
#+name: def:bijector
#+begin_definition :title ""
A *bijector* or *diffeomorphism* is a differentiable /bijection/ $b$ with a /differentiable/ inverse $b^{-1}$.
#+end_definition

#+ATTR_REVEAL: :frag (appear)
For example $b(x) = \exp(x)$
#+ATTR_REVEAL: :frag (appear)
- $\exp$ is differentiable
- $\exp$ has inverse $\log$
- $\log$ is differentiable (on $(0, \infty)$)
#+ATTR_REVEAL: :frag (appear)
So $\exp$ (and $\log$) is a bijector!

#+REVEAL: split

#+begin_src jupyter-julia :session jl :results silent :exports none
using Pkg; Pkg.activate("/home/tor/.julia/dev/Bijectors")
#+end_src

In =Bijectors.jl=

#+begin_src jupyter-julia :session jl :exports both :results none
using Bijectors; using Bijectors: Exp, Log

b = Exp()
b⁻¹ = inv(b)

b⁻¹ isa Log
#+end_src

#+RESULTS:
: true

#+HTML: <div class="fragment (appear)">
We can evaluate a =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
x = 0.0
b(x) == 1.0  # since e⁰ = 1
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We can /compose/ bijectors to get a new =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
(b ∘ b) isa Bijector
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

And evaluate compositions of bijectors

#+begin_src jupyter-julia :session jl :exports both
(b⁻¹ ∘ b)(x) == x
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
What about more complex/deeper compositions?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
cb = b ∘ b ∘ b
cb⁻¹ = inv(cb)        # <= inversion of a "large" composition

(cb⁻¹ ∘ cb)(x) == x
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We'll see later that of particular interest is the term

\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Which works seamlessly even for compositions

#+begin_src jupyter-julia :session jl :exports both
logabsdetjac(cb, x)
#+end_src

#+RESULTS:
: 3.718281828459045

#+HTML: </div>

** How does it relate to distributions?
Consider
\begin{equation*}
x \sim \mathcal{N}(1, 5)
\end{equation*}
#+HTML: <div class="fragment (appear)">
Or, equivalently,
\begin{equation*}
\begin{split}
  \xi &\sim \mathcal{N}(0, 1) \\
  x &:= 1 + 5 \xi
\end{split}
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Don't believe me?

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports none :results none
using Random; Random.seed!(42)
#+end_src

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-pdfs.svg :results none
d1 = Normal(1.0, 5.0)                # 𝒩(1, 5)
xs = rand(d1, 100_000)

d2 = Normal(0.0, 1.0)                # 𝒩(0, 1)
ys = 1.0 .+ rand(d2, 100_000) .* 5.0 # => y ~ 𝒩(1, 5)

density(xs, label = "d1", linewidth = 3)
density!(ys, label = "d2", linewidth = 3)
#+end_src

[[file:figures/normal-pdfs.svg]]

#+REVEAL: split

In =Bijectors.jl= we define transformations which _induce_ distributions

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
using Bijectors: Shift, Scale

# Define the transform
b = Shift(1.0) ∘ Scale(5.0) # => x ↦ 1.0 + 5.0x
td = transformed(d2, b)     # => 𝒩(1.0, 5.0)

y = rand(td)
# Ensure we have the same densities
logpdf(td, y) ≈ logpdf(d1, y)
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Moreover, =td= is a =TransformedDistribution= /and/

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
td isa Distribution
#+end_src

#+RESULTS:
: true

Yay!

#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports code :file figures/normal-transformed-pdfs.svg :results none
x_range = -14.0:0.05:16.0
plot(x_range, x -> pdf(d1, x), label = "d1", linewidth = 4, linestyle = :dash)
plot!(x_range, x -> pdf(td, x), label = "td", linewidth = 4, alpha = 0.6)
#+end_src

[[file:figures/normal-transformed-pdfs.svg]]

#+REVEAL: split

#+begin_quote
Great; just another way to draw samples from a normal distribution...
#+end_quote

Or /is it/?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports none
using Random; Random.seed!(10);
#+end_src

#+RESULTS:

#+begin_src jupyter-julia :session jl :exports code :file figures/simple-nf-pdf.svg :results none
d = MvNormal(zeros(1), ones(1))

# "Deep" transformation
b = (
    RadialLayer(1) ∘ RadialLayer(1) ∘
    RadialLayer(1) ∘ RadialLayer(1)
)
td = transformed(d, b)

# Sample from flow
xs = rand(td, 10_000)

x_range = minimum(xs):0.05:maximum(xs)
lps = pdf(td, reshape(x_range, (1, :)))  # compute the probabilities

histogram(vec(xs), bins = 100; normed = true, label = "", alpha = 0.7)
xlabel!("y")
plot!(x_range, lps, linewidth = 3, label = "p(y)")
#+end_src

#+HTML: </div>

#+REVEAL: split
#+LaTeX: \clearpage

#+RESULTS:
[[file:figures/simple-nf-pdf.svg]]

That doesn't look very /normal/, now does it?!

*** How?
It turns out that if $b$ is a =Bijector=, the process
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}
/induces/ a density $\tilde{p}(y)$ defined by
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}

#+HTML: <div class="fragment (appear)">
Therefore if we can compute $\mathcal{J}_{b^{-1}}(y)$ we can indeed compute $\tilde{p}(y)$!
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
But is this actually useful?
#+end_quote

** Normalising flows: parameterising $b$
One might consider constructing a /parameterised/ =Bijector= $b_{\phi}$.

#+HTML: <div class="fragment (appear)">
Given a density $p(x)$ we can obtain a parameterised density
\begin{equation*}
\tilde{p}_{\phi}(y) = p \big( b_{\phi}^{-1}(y) \big) \left| \det \mathcal{J}_{b_{\phi}^{-1}}(y) \right|
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
$b_{\phi}$ is often referred to as a *normalising flow (NF)*
#+end_quote

#+HTML: <div class="fragment (appear)">
Can now optimise any objective over distributions, e.g. perform /maximum likelihood estimation/ (MLE) for some given i.i.d. dataset $\left\{ y_i \right\}_{i = 1}^n$
\begin{equation*}
\underset{\phi}{\text{argmax}}\ \sum_{i = 1}^{n} \tilde{p}_{\phi}(y_i)
\end{equation*}
#+HTML: </div>

*** Example: MLE using NF
Consider an =Affine= transformation, i.e.
\begin{equation*}
\mathrm{aff}(x) = W x + b
\end{equation*}
for matrix $W$ and vector $b$,
#+HTML: <div class="fragment (appear)">
and a non-linear activation function, e.g. =LeakyReLU=
\begin{equation*}
a(x) = 
\begin{cases}
  x & \text{if } x \ge 0 \\
  \alpha x & \text{if } x < 0
\end{cases}
\end{equation*}
for some /non-zero/ $\alpha \in \mathbb{R}$ (usually chosen to be very small).
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Can define a "deep" NF by composing such transformations! Looks familiar?

#+HTML: <div class="fragment (appear)">
Yup; it's basically an invertible neural network! (assuming $\det W \ne 0$)

#+begin_src jupyter-julia :session jl
layers = [LeakyReLU(α[i]) ∘ Affine(W[i], b[i]) for i = 1:num_layers]

b = foldl(∘, layers)
td = transformed(base_dist, b)  # <= "deep" normalizing flow!
#+end_src

#+HTML: </div>

#+REVEAL: split

#+ATTR_HTML: :width 35%
#+CAPTION: Empirical density estimate (blue) compared with single batch of samples (red).
file:figures/nf-banana-density-estimation.gif

** Methods

|-------------------------------------------------------------+-------------------------+--------------|
| Operation                                                   | Method                  | Automatic    |
|-------------------------------------------------------------+-------------------------+--------------|
| $b \mapsto b^{-1}$                                          | =inv(b)=                | $\checkmark$ |
| $(b_1, b_2) \mapsto (b_1 \circ b_2)$                        | =b1 ∘ b2=               | $\checkmark$ |
| $(b_1, b_2) \mapsto [b_1, b_2]$                             | =stack(b1, b2)=         | $\checkmark$ |
| $(b, n) \mapsto b^n := b \circ \cdots \circ b$ (n times)    | =b^n=                   | $\checkmark$ |
|-------------------------------------------------------------+-------------------------+--------------|

#+REVEAL: split

|----------------------------------------------------------------+----------------------+--------------|
| Operation                                                      | Method               | Automatic    |
|----------------------------------------------------------------+----------------------+--------------|
| $x \mapsto b(x)$                                               | =b(x)=               | $\times$     |
| $y \mapsto b^{-1}(y)$                                          | =inv(b)(y)=          | $\times$     |
| $x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert$         | =logabsdetjac(b, x)= | AD           |
| $x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)$ | =forward(b, x)=      | $\checkmark$ |
|----------------------------------------------------------------+----------------------+--------------|

#+REVEAL: split

|--------------------------------------------------------------------+-------------------------+--------------|
| Operation                                                          | Method                  | Automatic    |
|--------------------------------------------------------------------+-------------------------+--------------|
| $p \mapsto q:= b_* p$                                              | =q = transformed(p, b)= | $\checkmark$ |
| $y \sim q$                                                         | =y = rand(q)=           | $\checkmark$ |
| $p \mapsto b$ s.t. $\mathrm{support}(b_* p) = \mathbb{R}^d$        | =bijector(p)=           | $\checkmark$ |
| $\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)$ | =forward(q)=            | $\checkmark$ |
|--------------------------------------------------------------------+-------------------------+--------------|


* Comparison
** NF-ADVI vs. MF-ADVI
Consider $L = \begin{pmatrix} 10 & 0 \\ 10 & 10 \end{pmatrix}$ and
\begin{equation*}
\begin{split}
  m & \sim \mathcal{N}(0, 1) \\
  x_i & \overset{i.i.d.}{=} \mathcal{N}(m, L L^T) \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+HTML: <div class="fragment (appear)">

In =Turing.jl=
#+begin_src jupyter-julia :session jl
using Turing

L = [
    10 0;
    10 10
]

@model demo(x) = begin
    μ ~ MvNormal(zeros(2), ones(2))

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(μ, L * transpose(L))
    end
end
#+end_src

#+HTML: </div>

*** Visualized
#+HTML: <div id="mvnormal-comparison">

#+CAPTION: True posterior
file:figures/mvnormal-1-posterior.png

#+CAPTION: MF-ADVI
file:figures/mvnormal-1-mfvi-elbo.png

#+HTML: <div class="fragment (appear)">

#+CAPTION: NF-ADVI (rational-quadratic)
file:figures/mvnormal-1-nfvi-elbo.png

#+CAPTION: NF-ADVI (affine)
file:figures/mvnormal-1-nfvi-elbo-affine.png

#+HTML: </div>

#+HTML: </div>

** Example: 8 schools dataset
#+begin_src jupyter-julia :session jl :exports code :results none
using Turing

# data
ys = Float64.([28, 8, -3, 7, -1, 1, 18, 12])
σs = Float64.([15, 10, 16, 11, 9, 11, 10, 18])

# model
@model eight_schools(y, σ) = begin
    n = length(y)
    
    τ ~ Truncated(Cauchy(0, 5), 0, Inf)
    μ ~ Normal(0, 5)

    θ ~ MvNormal(ones(n) * μ, ones(n) * τ)
    y ~ MvNormal(θ, σ)
end

m = eight_schools(ys, σs)
#+end_src

*** =NUTS= and =ADVI=
#+begin_src jupyter-julia :session jl :exports code
# HMC
initial_samples = 200
samples = sample(m, NUTS(10_000 + initial_samples, initial_samples, 0.65))

# ADVI
advi = ADVI(10, 10_000)
q = vi(m, advi)
samples_advi = rand(q, 10_000);
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
┌ Info: Found initial step size
│   init_ϵ = 0.8
└ @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
┌ Info: Finished 200 adapation steps
│   adaptor = StanHMCAdaptor(n_adapts=200, pc=DiagPreconditioner, ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.65, state.ϵ=0.22697565000809314), init_buffer=75, term_buffer=50)
│   τ.integrator = Leapfrog(ϵ=0.227)
│   h.metric = DiagEuclideanMetric([0.446261, 7.38653, 10.5031 ...])
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
┌ Info: Finished 10200 sampling steps in 7.147655536 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.446261, 7.38653, 10.5031 ...]))
│   τ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(ϵ=0.227), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 2372.3111561255632
│   average_acceptance_rate = 0.7981437762406471
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
┌ Info: [ADVI] Should only be seen once: optimizer created for θ
│   objectid(θ) = 2629648457811964606
└ @ Turing.Variational /home/tor/.julia/dev/Turing/src/variational/VariationalInference.jl:149
[32m[ADVI] Optimizing...  0%  ETA: 7:49:36[39m[32m[ADVI] Optimizing... 26%  ETA: 0:00:12[39m[32m[ADVI] Optimizing... 47%  ETA: 0:00:06[39m[32m[ADVI] Optimizing... 69%  ETA: 0:00:03[39m[32m[ADVI] Optimizing... 90%  ETA: 0:00:01[39m[32m[ADVI] Optimizing...100% Time: 0:00:07[39m
#+END_EXAMPLE

#+begin_src jupyter-julia :session jl :exports results :file figures/eight_schools_hmc_advi.svg
histogram(log.(samples_advi[1, :]), normed = true, bins = 100, alpha = 0.7, label = "ADVI")
density!(log.(samples_advi[1, :]), linewidth = 3, label = "")
histogram!(log.(vec(samples[:τ].value)), normed = true, bins = 100, alpha = 0.5, label = "NUTS")
#+end_src

#+RESULTS:
[[file:figures/eight_schools_hmc_advi.svg]]

*** NF-ADVI
#+begin_src jupyter-julia :session jl :exports both
using Turing: Variational
Variational.elbo(advi, q, m, 1000)
#+end_src

#+RESULTS:
: -33.54428894047941

#+begin_src jupyter-julia :session jl
using LinearAlgebra

using Turing: Variational

using Bijectors: CouplingLayer, PartitionMask, Shift, Scale

using Flux
using Flux: @treelike, Optimise

@treelike CouplingLayer
@treelike Composed
@treelike Shift
@treelike Scale
@treelike TransformedDistribution

# TODO: NF-ADVI
var_info = Turing.VarInfo(m);
sym2range = Dict()
ranges = []
dists = []

start = 0

for k in keys(var_info.metadata)
    md = var_info.metadata[k]
    sym_ranges = []
    
    for i in 1:length(md.dists)
        d = md.dists[i]
        r = md.ranges[i]

        push!(sym_ranges, r .+ start)

        push!(dists, d)
        push!(ranges, r .+ start)
        start += r[end]
    end

    sym2range[k] = sym_ranges
end

sym2range

bs = inv.(bijector.(tuple(dists...)))
sb = Stacked(bs, ranges)

# TODO: Define NF
num_latent = ranges[end][end]
num_θ = length(vcat(collect.(sym2range[:θ])...))
b_parameterized = RadialLayer(num_latent)


b = (sb ∘ b_parameterized)
base = MvNormal(zeros(num_latent), ones(num_latent))
flow = transformed(base, b)

rand(flow)

# Coupling flow
nn_θ2τ = Chain(Dense(1, 8, relu), Dense(8, 2 * num_θ))
mask_θ2τ = PartitionMask(
    num_latent,
    vcat(collect.(sym2range[:θ])...),
    vcat(collect.(sym2range[:τ])...),
    vcat(collect.(sym2range[:μ])...)
)
cl_θ2τ = CouplingLayer(θ -> Affine(exp.(θ[1:8]) .+ 0.1, θ[9:16]), mask_θ2τ, nn_θ2τ)

nn_θ2μ = Chain(Dense(1, 8, relu), Dense(8, 2 * num_θ))
mask_θ2μ = PartitionMask(
    num_latent,
    vcat(collect.(sym2range[:θ])...),
    vcat(collect.(sym2range[:μ])...),
    vcat(collect.(sym2range[:τ])...)
)

cl_θ2μ = CouplingLayer(θ -> Affine(exp.(θ[1:8]) .+ 0.1, θ[9:16]), mask_θ2μ, nn_θ2μ)

# Trying it out
cl_θ2μ(rand(flow))
(cl_θ2τ ∘ cl_θ2μ)(rand(flow))

# Defining the actual flow
b = sb ∘ cl_θ2μ ∘ cl_θ2τ
base = Turing.Core.TuringDiagNormal(zeros(num_latent), ones(num_latent))
flow = transformed(base, b)

# check that everything works
logpdf(flow, rand(flow, 10))

x, y, logjac, logq = forward(flow, 3)

# Affine transform

Affine(W, b; dim::Type{Val{N}} = Val{1}) where {N} = Shift(b; dim = Val{N}) ∘ Scale(W; dim = Val{N})

μ̂ = param(randn(num_latent) ./ sqrt(num_latent))
logσ̂ = param(randn(num_latent) ./ sqrt(num_latent))


# So objective is nothing more than
elbo(q, logjoint, logjac) = mean(logjoint + logjac) + entropy(q)

opt = Variational.DecayedADAGrad(1e-4)
num_samples = 10
num_iters = 10_000

using OnlineStats
elbo_mean = Mean(weight = ExponentialWeight(1e-4))

using ProgressMeter
prog = ProgressMeter.Progress(num_iters)

for epoch = 1:num_iters
    aff = Affine(exp.(logσ̂), μ̂)
    b = sb ∘ cl_θ2μ ∘ cl_θ2τ ∘ aff
    flow = transformed(base, b)
    
    _, y, logjac, _ = forward(flow, num_samples)
    logjoint = Tracker.collect([Variational.logdensity(m, var_info, y[:, i]) for i = 1:num_samples])
    
    obj = - elbo(flow, logjoint, logjac)
    Tracker.back!(obj, 1.0)
    # @info Tracker.data(obj)

    ∇_norm = 0.0

    # update params
    ∇_norm += norm(Tracker.grad(μ̂))
    Optimise.update!(opt, μ̂, Tracker.grad(μ̂))
    ∇_norm += norm(Tracker.grad(logσ̂))
    Optimise.update!(opt, logσ̂, Tracker.grad(logσ̂))

    for p in Flux.params(cl_θ2τ)
        ∇_norm += norm(Tracker.grad(p))
        Optimise.update!(opt, p, Tracker.grad(p))
    end

    for p in Flux.params(cl_θ2μ)
        ∇_norm += norm(Tracker.grad(p))
        Optimise.update!(opt, p, Tracker.grad(p))
    end

    fit!(elbo_mean, -Tracker.data(obj))
    ProgressMeter.next!(
        prog;
        showvalues = [
            (:elbo, -Tracker.data(obj)),
            (:elbo_mean, value(elbo_mean)),
            (:∇_norm, ∇_norm)]
    )
end

ẑ = mean(Tracker.data(rand(flow, 100)); dims = 2)
log(ẑ[1])

zs = hcat([Tracker.data(rand(flow, 1000)) for i = 1:10]...)
histogram(log.(zs[1, :]))

θ_samples = zs[vec(collect.(sym2range[:θ])...), :]
mean(θ_samples; dims = 2)

histogram(θ_samples[1, :])
#+end_src

* Combining EVERYTHING

** Simple non-diagonal Gaussian (cont.)

#+begin_src jupyter-julia :session jl-nf-advi :exports none :results none
# SETUP
using Flux
using Flux: Optimise
using LinearAlgebra, Random
using StatsFuns: softplus

using ProgressMeter

using Plots, StatsPlots

using ForwardDiff
using Logging

using Pkg; Pkg.activate("~/.julia/dev/Bijectors")

using Bijectors
using Bijectors: Shift, Scale, CouplingLayer, PartitionMask

using Turing
using Turing: Variational
using Turing.Core: TuringDiagNormal

rng = Random.seed!(1)

# Variational inference stuff
Affine(W, b; dim::Type{Val{N}} = Val{1}) where {N} = Shift(b; dim = Val{N}) ∘ Scale(W; dim = Val{N})
g(θ) = Affine(exp(θ[1]) + 0.1, θ[2]; dim = Val{1})

function build_bijector(θ)
    σ = exp.(θ[1:2]) .+ 0.01
    μ = θ[3:4]

    W1 = reshape(θ[5:6], (2, 1))
    b1 = θ[7:8]
    W2 = reshape(θ[9:12], (2, 2))
    b2 = θ[13:14]

    nn1 = Chain(Dense(W1, b1, relu), Dense(W2, b2))

    W1 = reshape(θ[15:16], (2, 1))
    b1 = θ[17:18]
    W2 = reshape(θ[19:22], (2, 2))
    b2 = θ[23:24]

    nn2 = Chain(Dense(W1, b1, relu), Dense(W2, b2))

    mask1 = PartitionMask(2, [2], [1], nothing)
    cl1 = CouplingLayer(g, mask1, nn1)
    
    mask2 = PartitionMask(2, [1], [2], nothing)
    cl2 = CouplingLayer(g, mask2, nn2)

    return cl2 ∘ cl1 ∘ Affine(σ, μ)
end

# Posterior
L = [1. 0.; 1. 1.] .* 10
prior = MvNormal(zeros(2), ones(2))
likelihood = MvNormal(zeros(2), L * L')

# Data
n = 100
xs = rand(likelihood, n)

Σ = inv(inv(prior.Σ) + n * inv(likelihood.Σ))
μ = vec(Σ * (inv(prior.Σ) * prior.μ + n * inv(likelihood.Σ) * mean(xs; dims = 2)))
posterior = MvNormal(μ, Σ)

samples_posterior = rand(posterior, 1000)

# Turing model
@model gaussian(xs) = begin
    μ ~ prior

    for i = 1:size(xs, 2)
        xs[:, i] ~ MvNormal(μ, L * L')
    end
end

m = gaussian(xs)

# NF
BATCH_SIZE = 50
NUM_OPT_STEPS = 1_000

var_info = Turing.VarInfo(m)
function f(θ)
    b = build_bijector(θ)
    q = transformed(base, b)

    z₀, zₖ, logjac, logq = forward(q, BATCH_SIZE)
    lp = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:BATCH_SIZE]
    return - mean(lp + logjac) #ELBO
end

# initialization
base = Turing.Core.TuringDiagNormal(zeros(2), ones(2))

num_params = 34
θ = randn(num_params) ./ sqrt(num_params)

# optimize
opt = Variational.DecayedADAGrad(1e-3)

steps_taken = 0
prog = ProgressMeter.Progress(NUM_OPT_STEPS)

# initial ELBO
b = build_bijector(θ)
q = transformed(base, b)

z₀, zₖ, logjac, logq = forward(q, 100)
prev_logq = mean(logq)
prev_logjac = mean(logjac)

logp = logpdf(posterior, zₖ)
prev_logp = mean(logp)

logjoint = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:size(zₖ, 2)]
prev_logjoint = mean(logjoint)

prev_kl = mean(logq - logjoint)
prev_elbo = mean(logjoint + logjac) + entropy(q)


for i = 1:NUM_OPT_STEPS
    Δ = ForwardDiff.gradient(f, θ)

    ∇_norm = norm(Δ)
    Optimise.apply!(opt, θ, Δ)

    θ = θ .- Δ

    if i % 10 == 0
        b = build_bijector(θ)
        q = transformed(base, b)

        z₀, zₖ, logjac, logq = forward(q, 100)
        prev_logq = mean(logq)
        prev_logjac = mean(logjac)
        logp = logpdf(posterior, zₖ)
        prev_logp = mean(logp)
        
        prev_kl = mean(logq - logp)

        logjoint = [Variational.logdensity(m, var_info, zₖ[:, i]) for i = 1:100]
        prev_logjoint = mean(logjoint)
        prev_elbo = mean(logjoint + logjac) + entropy(q)

    end

    ProgressMeter.next!(
        prog;
        showvalues = [
            (:kl, prev_kl),
            (:elbo, prev_elbo),
            (:∇_norm, ∇_norm),
            (:prev_logjac, prev_logjac),
            (:prev_logp, prev_logp),
            (:prev_logjoint, prev_logjoint),
            (:prev_logq, prev_logq)
        ]
    )
end
θ_elbo = copy(θ)
b = build_bijector(θ_elbo)
q_nf_elbo = transformed(base, b)
samples_nf_elbo = rand(q_nf_elbo, 1000)

# visaulize
x_range = -2.0:0.1:1.5
y_range = -2.5:0.1:2.0

contour(x_range, y_range, (x, y) -> pdf(posterior, [x, y]), label = "Posterior")
contour!(x_range, y_range, (x, y) -> pdf(q_nf_elbo, [x, y]), label = "NF-ADVI")

# Okay, so now we use this as a prior
flow = q_nf_elbo;
#+end_src

#+begin_src jupyter-julia :session jl-nf-advi :exports none
flow # <= flow from earlier

L = [
    10. 0.;
    10. 10.
]
#+end_src

We change the true mean $\mu$ from =[0, 0]= to =[0.5, 0.5]=

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
# Data generation
n = 100
μ_true = 0.5 .* ones(2)  # <= different from original problem
likelihood = MvNormal(μ_true, L * transpose(L))
xs = rand(likelihood, n)
#+end_src

#+RESULTS:
: 2×100 Array{Float64,2}:
:  -15.3813  3.52882  -5.81578  -19.7243  …  -1.26671    8.8258   -5.49379
:  -12.4074  2.51113   0.42283  -26.9775     -0.724036  19.7033  -17.2421 

#+HTML: <div class="fragment (appear)">

And add the normalizing flow variational posterior from before as a prior

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
@model flow_demo(x) = begin
    μ ~ flow # <= NF-ADVI applied to `MvNormal` from before

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(μ, L * transpose(L))
    end
end
#+end_src 

#+HTML: </div>

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports none
prior = posterior # <= because we use previous posterior

Σ = inv(inv(prior.Σ) + n * inv(likelihood.Σ))
μ = vec(Σ * (inv(prior.Σ) * prior.μ + n * inv(likelihood.Σ) * mean(xs; dims = 2)))
posterior = MvNormal(μ, Σ)

samples_posterior = rand(posterior, 10_000)
#+end_src

#+RESULTS:
: 2×10000 Array{Float64,2}:
:  -0.15542  -0.174742   0.211021  …   0.550523  -0.623962   0.2692  
:  -1.64341  -0.205921  -0.686019     -1.4739    -1.64327   -0.174998

#+REVEAL: split

Then we perform exact inference using MCMC

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports code
# Sample
m = flow_demo(xs)
samples = sample(m, NUTS(20_000, 1000, 0.65))
#+end_src

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
┌ Info: Found initial step size
│   init_ϵ = 0.8
└ @ Turing.Inference /home/tor/.julia/dev/Turing/src/inference/hmc.jl:365
┌ Info: Finished 1000 adapation steps
│   adaptor = StanHMCAdaptor(n_adapts=1000, pc=DiagPreconditioner, ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.65, state.ϵ=0.9161520150457593), init_buffer=75, term_buffer=50)
│   τ.integrator = Leapfrog(ϵ=0.916)
│   h.metric = DiagEuclideanMetric([0.256373, 0.441133])
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:145
┌ Info: Finished 20000 sampling steps in 115.067615846 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.256373, 0.441133]))
│   τ = NUTS{Multinomial,Generalised}(integrator=Leapfrog(ϵ=0.916), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 23143.857593560853
│   average_acceptance_rate = 0.8397687531321688
└ @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
#+END_EXAMPLE
#+BEGIN_EXAMPLE
Object of type Chains, with data of type 19000×12×1 Array{Union{Missing, Float64},3}

Log evidence      = 0.0
Iterations        = 1:19000
Thinning interval = 1
Chains            = 1
Samples per chain = 19000
internals         = acceptance_rate, eval_num, hamiltonian_energy, is_accept, log_density, lp, n_steps, numerical_error, step_size, tree_depth
parameters        = μ[1], μ[2]

2-element Array{ChainDataFrame,1}

Summary Statistics
. Omitted printing of 1 columns
│ Row │ parameters │ mean      │ std      │ naive_se   │ mcse       │ ess     │
│     │ [90mSymbol[39m     │ [90mFloat64[39m   │ [90mFloat64[39m  │ [90mFloat64[39m    │ [90mFloat64[39m    │ [90mAny[39m     │
├─────┼────────────┼───────────┼──────────┼────────────┼────────────┼─────────┤
│ 1   │ μ[1]       │ -0.240894 │ 0.526839 │ 0.00382209 │ 0.00519204 │ 11004.0 │
│ 2   │ μ[2]       │ -0.565251 │ 0.676478 │ 0.00490768 │ 0.00689196 │ 10801.6 │

Quantiles

│ Row │ parameters │ 2.5%     │ 25.0%     │ 50.0%     │ 75.0%     │ 97.5%    │
│     │ [90mSymbol[39m     │ [90mFloat64[39m  │ [90mFloat64[39m   │ [90mFloat64[39m   │ [90mFloat64[39m   │ [90mFloat64[39m  │
├─────┼────────────┼──────────┼───────────┼───────────┼───────────┼──────────┤
│ 1   │ μ[1]       │ -1.27603 │ -0.596539 │ -0.238923 │ 0.123116  │ 0.7855   │
│ 2   │ μ[2]       │ -1.91775 │ -1.0214   │ -0.558174 │ -0.107487 │ 0.743114 │
#+END_EXAMPLE
:END:

#+HTML: <div class="fragment (appear)">

#+begin_src jupyter-julia jupyter-julia :session jl-nf-advi :exports results :file figures/flow-in-generative-model.svg
samples_vals = samples[:μ].value

x_range = -2.0:0.1:2.0
y_range = -2.5:0.1:2.0

histogram2d(samples_vals[:, 1], samples_vals[:, 2], bins = 25, normed = true, fill = cgrad(:viridis))
contour!(x_range, y_range, (x, y) -> pdf(posterior, [x, y]), label = "Posterior", linecolor = cgrad(:deep), linewidth = 3)
#+end_src

#+CAPTION: Results of using =NUTS= to sample from generative model with NF prior. Contour plot is of /true/ posterior, while histogram is of the =NUTS= samples.
#+RESULTS:
[[file:figures/flow-in-generative-model.svg]]

#+HTML: </div>

** Use case
- Suppose you have a large dataset $\mathcal{Z} = \left\{ z_i \right\}_{i = 1}^m$
- Then, in some other setting, maybe just a different experiment, you get observations $\mathcal{X} = \left\{ x_i \right\}_{i = 1}^n$ for some other random variable $x$
- You have prior knowledge that indeed $x$ depends on $z$, but you're not sure what prior to use for $z$

** Example: selling ice cream /optionally/
Example: you have two /different/ datasets
- $\mathcal{Z} := \left\{ z_j \right\}_{j = 1}^m$, locations of the j-th person in some country → $p(z)$ is a "population density"
- $\mathcal{X} := \left\{ x_i \right\}_{i = 1}^n$, integer labels corresponding to the ice cream vendor from which the i-th person purchased ice cream
- $m$ and $n$ are not necessarily the same, and these datasets are obtained in two different scenarios, maybe two different days

You own a series of $k$ ice cream parlours on the beach, but this is not enough for you. You want to expand your business into also selling fake sunglasses. You want your sellers to annoyingly walk around and ask people if they want these sunglasses, but obviously you want to make this as efficient as possible.

You have a dataset $\mathcal{X} = \left\{ x_i \right\}_{i = 1}^N$ of ice creams sold, and, you were quite lucky on this one, you found a hacker which could sell you GPS data $\mathcal{Z} = \left\{ z_j \right\}_{j = 1}^M$ of all the people on the beach! Unfortunately this was just a single snapshot of the locations of all the $M$ people on the beach at a single time, but this was in the middle of the day so you think it is a reasonable representation of the location-density.

Unfortunately you have /a lot/ of GPS data but not so much ice cream data. What you would really like to ask is "where are people located when they go and buy ice cream?", or rather, "what is $p(z \mid x = 1)$?". If you knew this, then you could put your on-the-move napkin-salesmen in those high-probability-of-purchasing-ice-cream locations! This way you can be there when they all inevitably need napkins after eating their ice-cream. 


*** Approach
1. Density estimation using $\mathcal{Z}$ to get a population density $p(z)$
2. Define model for ice cream purchases
   \begin{equation*}
   \begin{split}
     z & \sim \hat{p}(z \mid \mathcal{Z}) \\
     \pi & := f(z) \\
     x_i & \sim \mathrm{Categorical}(\pi), \quad i = 1, \dots, n
   \end{split}
   \end{equation*}
   where $f$ maps location $z$ to probabilities over the different ice cream parlors on the beach.
3. Perform exact inference using a MCMC method, e.g. =ParticleGibbs=


** How
- Maybe you have some highly complex data for which you don't have, nor want to make any, assumptions
- Can do density estimation of the data using a normalizing flow!

#+begin_src jupyter-julia :session jl
d = 10

dist = MvNormal(zeros(d), ones(d))
b = PlanarFlow(d) ∘ RadialFlow(d) ∘ PlanarFlow(d) ∘ RadialFlow(d)
flow = transformed(dist, b)

# TODO: do density estimation using `flow`

# TODO: define a model which uses the learned `flow`
@model flow_based(x) = begin
    z ~ flow

    for i = size(x, 2)
        x[:, i] ~ MvNormal(z, ones(d))
    end
end


# TODO: generate data
m = flow_based(data)
samples = sample(m, HMC(1000, 0.1, 10); progress = true)
#+end_src

