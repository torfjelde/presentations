#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_TITLE_SLIDE: <div><h1>You have data and I have distributions</h1><h3>A talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></h3></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: moon

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: You have data and I have distributions: a talk on =Turing.jl= and =Bijectors.jl=

* Overview
1. Bayesian inference and density estimation
   - Why you want to do Bayesian inferenece
   - What it means to do Bayesian inference
   - Brief mention of what "density estimation" refers to
2. Bayesian and approximate inference in =Turing.jl=
   - Bayesian inference on simple example
   - =ADVI= on sample example
3. =Bijectors.jl=: what is it and why do we care?
4. Normalizing flow VI
5. Combining EVERYTHING

* Why bother?
*Goal:* find $\theta$ which maximises the probability of our observations $\left\{ x_i \right\}_{i = 1}^n$

** Bayes' rule
*Bayes' rule* gives us
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big) = \frac{p\big(\left\{ x_i \right\}_{i = 1}^n \mid \theta\big) p(\theta)}{p\big(\left\{ x_i \right\}_{i = 1}^n\big)}
\end{equation*}
or, since the denominator is constant,
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big) \propto p \big( \left\{ x_i \right\}_{i = 1}^n \mid \theta \big) p(\theta)
\end{equation*}

** Reasons
- "But Tor, why do I need distributions for my data? Why is it not enough with just a /single point/ for my predictions?"
- Well then I'll ask you; do you want to know how certain you are about your 


* Setup
#+begin_src jupyter-julia :session jl :exports code :results silent
using Plots, StatsPlots
#+end_src

#+begin_src jupyter-julia :session jl :exports code :results silent :async yes
using Turing
#+end_src


* =Turing.jl=
#+begin_quote
=Turing.jl= is a (universal) _probabilistic programming language_ (PPL) in Julia.
#+end_quote

#+ATTR_REVEAL: :frag (appear)
What does that even mean?

#+ATTR_REVEAL: :frag (appear)
1. Specify generative model in Julia with neat syntax
2. Perform Bayesian inference over the latent variables using a vast library of MCMC samplers
3. ???
4. Profit!!!

** Gaussian-InverseGamma conjugate model
#+ATTR_REVEAL: :frag (appear)
In mathematical notation:
#+ATTR_REVEAL: :frag (appear)
\begin{equation*}
\begin{split}
  s & \sim \mathrm{InverseGamma}(2, 3) \\
  m & \sim \mathcal{N}(0, \sqrt{s}) \\
  x_i & \sim \mathcal{N}(m, \sqrt{s}), \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+ATTR_REVEAL: :frag (appear)
In =Turing.jl=:
#+ATTR_REVEAL: :frag (appear)
#+begin_src jupyter-julia :session jl
@model model(x) = begin
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, ‚àös)
    for i = 1:length(x)
        x[i] ~ Normal(m, ‚àös)
    end
end
#+end_src

** TODO =HMC=

#+begin_src jupyter-julia :session jl
xs = randn(1000)

# Sample 1000 samples using HMC
samples_nuts = sample(xs, HMC(1000, 0.1, 10))
#+end_src

#+begin_src jupyter-julia :session jl :file figures/gaussian-inversegamma-hmc.svg
plot(samples_nuts[[:s, :m]])
#+end_src

** =ADVI=
[[./figures/advi_w_elbo_fps15_125_forward_diff.gif]]

** Benchmarks
file:figures/turing-benchmarks.png

* =Bijectors.jl=
#+ATTR_REVEAL: :frag (appear)
#+name: def:bijector
#+begin_definition :title ""
A *bijector* or *diffeomorphism* is a differentiable /bijection/ $b$ with a /differentiable/ inverse $b^{-1}$.
#+end_definition

#+ATTR_REVEAL: :frag (appear)
For example $b(x) = \exp(x)$
#+ATTR_REVEAL: :frag (appear)
- $\exp$ is differentiable
- $\exp$ has inverse $\log$
- $\log$ is differentiable (on $(0, \infty)$)
#+ATTR_REVEAL: :frag (appear)
So $\exp$ (and $\log$) is a bijector!

#+REVEAL: split

#+begin_src jupyter-julia :session jl :results silent :exports none
using Pkg; Pkg.activate("/home/tor/.julia/dev/Bijectors")
#+end_src

In =Bijectors.jl=

#+begin_src jupyter-julia :session jl :exports both :results none
using Bijectors; using Bijectors: Exp, Log

b = Exp()
b‚Åª¬π = inv(b)

b‚Åª¬π isa Log
#+end_src

#+RESULTS:
: true

#+HTML: <div class="fragment (appear)">
We can evaluate a =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
x = 0.0
b(x) == 1.0  # since e‚Å∞ = 1
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We can /compose/ bijectors to get a new =Bijector=

#+begin_src jupyter-julia :session jl :exports both :results none
(b ‚àò b) isa Bijector
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+REVEAL: split

And evaluate compositions of bijectors

#+begin_src jupyter-julia :session jl :exports both
(b‚Åª¬π ‚àò b)(x) == x
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
What about more complex/deeper compositions?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
cb = b ‚àò b ‚àò b
cb‚Åª¬π = inv(cb)        # <= inversion of a "large" composition

(cb‚Åª¬π ‚àò cb)(x) == x
#+end_src

#+RESULTS:
: true

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
We'll see later that of particular interest is the term

\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Which works seamlessly even for compositions

#+begin_src jupyter-julia :session jl :exports both
logabsdetjac(cb, x)
#+end_src

#+RESULTS:
: 3.718281828459045

#+HTML: </div>

** How does it relate to distributions?
Consider
\begin{equation*}
x \sim \mathcal{N}(1, 5)
\end{equation*}
#+HTML: <div class="fragment (appear)">
Or, equivalently,
\begin{equation*}
\begin{split}
  \xi &\sim \mathcal{N}(0, 1) \\
  x &:= 1 + 5 \xi
\end{split}
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
Don't believe me?

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports none :results none
using Random; Random.seed!(42)
#+end_src

#+begin_src jupyter-julia :session jl :exports both :file figures/normal-pdfs.svg :results none
d1 = Normal(1.0, 5.0)                # ùí©(1, 5)
xs = rand(d1, 100_000)

d2 = Normal(0.0, 1.0)                # ùí©(0, 1)
ys = 1.0 .+ rand(d2, 100_000) .* 5.0 # => y ~ ùí©(1, 5)

density(xs, label = "d1", linewidth = 3)
density!(ys, label = "d2", linewidth = 3)
#+end_src

#+RESULTS:
[[file:figures/normal-pdfs.svg]]

#+REVEAL: split

In =Bijectors.jl= we define transformations which _induce_ distributions

#+begin_src jupyter-julia :session jl :exports both
using Bijectors: Shift, Scale

# Define the transform
b = Shift(1.0) ‚àò Scale(5.0) # => x ‚Ü¶ 1.0 + 5.0x
td = transformed(d2, b)     # => ùí©(1.0, 5.0)

y = rand(td)
# Ensure we have the same densities
logpdf(td, y) ‚âà logpdf(d1, y)
#+end_src

#+RESULTS:
: true

#+ATTR_REVEAL: :frag (appear)
Moreover, =td= is a =TransformedDistribution= and

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports both
td isa Distribution
#+end_src

#+RESULTS:
: true

Yay!

#+HTML: </div>

#+REVEAL: split

#+begin_src jupyter-julia :session jl :exports both :file figures/normal-transformed-pdfs.svg :results none
x_range = -14.0:0.05:16.0
plot(x_range, x -> exp(logpdf(d1, x)), label = "d1", linewidth = 4, linestyle = :dash)
plot!(x_range, x -> exp(logpdf(td, x)), label = "td", linewidth = 4, alpha = 0.6)
#+end_src

#+RESULTS:
[[file:figures/normal-transformed-pdfs.svg]]

#+REVEAL: split

#+begin_quote
Great; just another way to draw samples from a normal distribution...
#+end_quote

Or /is it/?

#+HTML: <div class="fragment (appear)">
#+begin_src jupyter-julia :session jl :exports none
using Random; Random.seed!(10);
#+end_src

#+RESULTS:

#+begin_src jupyter-julia :session jl :exports code :file figures/simple-nf-pdf.svg :results none
d = MvNormal(zeros(1), ones(1))

# "Deep" transformation
b = (
    RadialLayer(1) ‚àò RadialLayer(1) ‚àò
    RadialLayer(1) ‚àò RadialLayer(1)
)
td = transformed(d, b)

# Sample from flow
xs = rand(td, 10_000)

x_range = minimum(xs):0.05:maximum(xs)
lps = exp.(logpdf(td, reshape(x_range, (1, :))))  # compute the probabilities

histogram(vec(xs), bins = 100; normed = true, label = "", alpha = 0.7)
xlabel!("y")
plot!(x_range, lps, linewidth = 3, label = "p(y)")
#+end_src

#+HTML: </div>

#+REVEAL: split
#+LaTeX: \clearpage

#+RESULTS:
[[file:figures/simple-nf-pdf.svg]]

That doesn't look very /normal/, now does it?!

*** How?
It turns out that the process
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}
for a =Bijector= $b$ defines a density $\tilde{p}(y)$
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}

#+HTML: <div class="fragment (appear)">
Therefore if we can compute $\mathcal{J}_{b^{-1}}(y)$ we can indeed compute $\tilde{p}(y)$!
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
But is this actually useful?
#+end_quote

** Normalising flows: parameterising $b$
One might consider constructing a /parameterised/ =Bijector= $b_{\phi}$.

#+HTML: <div class="fragment (appear)">
Given a density $p(x)$ we can obtain a parameterised density
\begin{equation*}
\tilde{p}_{\phi}(y) = p \big( b_{\phi}^{-1}(y) \big) \left| \det \mathcal{J}_{b_{\phi}^{-1}}(y) \right|
\end{equation*}
#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
#+begin_quote
$b_{\phi}$ is often referred to as a *normalising flow (NF)*
#+end_quote

#+HTML: <div class="fragment (appear)">
Can now optimise any objective over distributions, e.g. perform /maximum likelihood estimation/ (MLE) for some given i.i.d. dataset $\left\{ y_i \right\}_{i = 1}^n$
\begin{equation*}
\underset{\phi}{\text{argmax}}\ \sum_{i = 1}^{n} \tilde{p}_{\phi}(y_i)
\end{equation*}
#+HTML: </div>

*** Example: MLE using NF
#+ATTR_HTML: :width 35%
#+CAPTION: Empirical density estimate (blue) compared with single batch of samples (red).
file:figures/nf-banana-density-estimation.gif

** Methods

|-------------------------------------------------------------+-------------------------+--------------|
| Operation                                                   | Method                  | Automatic    |
|-------------------------------------------------------------+-------------------------+--------------|
| $b \mapsto b^{-1}$                                          | =inv(b)=                | $\checkmark$ |
| $(b_1, b_2) \mapsto (b_1 \circ b_2)$                        | =b1 ‚àò b2=               | $\checkmark$ |
| $(b_1, b_2) \mapsto [b_1, b_2]$                             | =stack(b1, b2)=         | $\checkmark$ |
| $(b, n) \mapsto b^n := b \circ \cdots \circ b$ (n times)    | =b^n=                   | $\checkmark$ |
|-------------------------------------------------------------+-------------------------+--------------|

#+REVEAL: split

|----------------------------------------------------------------+----------------------+--------------|
| Operation                                                      | Method               | Automatic    |
|----------------------------------------------------------------+----------------------+--------------|
| $x \mapsto b(x)$                                               | =b(x)=               | $\times$     |
| $y \mapsto b^{-1}(y)$                                          | =inv(b)(y)=          | $\times$     |
| $x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert$         | =logabsdetjac(b, x)= | AD           |
| $x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)$ | =forward(b, x)=      | $\checkmark$ |
|----------------------------------------------------------------+----------------------+--------------|

#+REVEAL: split

|--------------------------------------------------------------------+-------------------------+--------------|
| Operation                                                          | Method                  | Automatic    |
|--------------------------------------------------------------------+-------------------------+--------------|
| $p \mapsto q:= b_* p$                                              | =q = transformed(p, b)= | $\checkmark$ |
| $y \sim q$                                                         | =y = rand(q)=           | $\checkmark$ |
| $p \mapsto b$ s.t. $\mathrm{support}(b_* p) = \mathbb{R}^d$        | =bijector(p)=           | $\checkmark$ |
| $\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)$ | =forward(q)=            | $\checkmark$ |
|--------------------------------------------------------------------+-------------------------+--------------|


* Comparison
** NF-ADVI vs. MF-ADVI
Consider $L = \begin{pmatrix} 10 & 0 \\ 10 & 10 \end{pmatrix}$ and
\begin{equation*}
\begin{split}
  m & \sim \mathcal{N}(0, 1) \\
  x_i & \overset{i.i.d.}{=} \mathcal{N}(m, L L^T) \quad i = 1, \dots, n
\end{split}
\end{equation*}

#+HTML: <div class="fragment (appear)">

In =Turing.jl=
#+begin_src jupyter-julia :session jl
using Turing

L = [
    10 0;
    10 10
]

@model demo(x) = begin
    Œº ~ MvNormal(zeros(2), ones(2))

    for i = 1:size(x, 2)
        x[:, i] ~ MvNormal(Œº, L * transpose(L))
    end
end
#+end_src

#+HTML: </div>

*** Visualized
#+HTML: <div id="mvnormal-comparison">

#+CAPTION: True posterior
file:figures/mvnormal-1-posterior.png

#+CAPTION: MF-ADVI
file:figures/mvnormal-1-mfvi-elbo.png

#+HTML: <div class="fragment (appear)">

#+CAPTION: NF-ADVI (rational-quadratic)
file:figures/mvnormal-1-nfvi-elbo.png

#+CAPTION: NF-ADVI (affine)
file:figures/mvnormal-1-nfvi-elbo-affine.png

#+HTML: </div>

#+HTML: </div>

** Example: 8 schools dataset
#+begin_src jupyter-julia :session jl :exports code :results none
using Turing

# data
ys = Float64.([28, 8, -3, 7, -1, 1, 18, 12])
œÉs = Float64.([15, 10, 16, 11, 9, 11, 10, 18])

# model
@model eight_schools(y, œÉ) = begin
    n = length(y)
    
    œÑ ~ Truncated(Cauchy(0, 5), 0, Inf)
    Œº ~ Normal(0, 5)

    Œ∏ ~ MvNormal(ones(n) * Œº, ones(n) * œÑ)
    y ~ MvNormal(Œ∏, œÉ)
end

m = eight_schools(ys, œÉs)
#+end_src

*** =HMC= and =ADVI=
#+begin_src jupyter-julia :session jl :exports code
# HMC
samples = sample(m, HMC(10000, 0.1, 10))

# ADVI
advi = ADVI(10, 10000)
q = vi(m, advi)
samples_advi = rand(q, 10000);
#+end_src

#+RESULTS:
: ‚îå Info: Finished 10000 sampling steps in 1.569982444 (s)
: ‚îÇ   h = Hamiltonian(metric=UnitEuclideanMetric([1.0, 1.0, 1.0, 1.0, 1.0, 1 ...]))
: ‚îÇ   œÑ = StaticTrajectory(integrator=Leapfrog(œµ=0.1), Œª=10))
: ‚îÇ   EBFMI_est = 1557.7643702458263
: ‚îÇ   average_acceptance_rate = 0.9592029163627042
: ‚îî @ AdvancedHMC /home/tor/.julia/packages/AdvancedHMC/n85yG/src/sampler.jl:157
: ‚îå Info: [ADVI] Should only be seen once: optimizer created for Œ∏
: ‚îÇ   objectid(Œ∏) = 16025309464792552139
: ‚îî @ Turing.Variational /home/tor/.julia/dev/Turing/src/variational/VariationalInference.jl:149
: [32m[ADVI] Optimizing...  0%  ETA: 7:48:59[39m[32m[ADVI] Optimizing... 23%  ETA: 0:00:14[39m[32m[ADVI] Optimizing... 44%  ETA: 0:00:07[39m[32m[ADVI] Optimizing... 65%  ETA: 0:00:03[39m[32m[ADVI] Optimizing... 86%  ETA: 0:00:01[39m[32m[ADVI] Optimizing...100% Time: 0:00:07[39m

#+begin_src jupyter-julia :session jl :exports results :file figures/eight_schools_hmc_advi.svg
histogram(log.(samples_advi[1, :]), normed = true, bins = 100, alpha = 0.7)
density!(log.(samples_advi[1, :]), linewidth = 3)
histogram!(log.(vec(samples[:œÑ].value)), normed = true, bins = 100, alpha = 0.5)
#+end_src

#+RESULTS:
[[file:figures/eight_schools_hmc_advi.svg]]

*** NF-ADVI
#+begin_src jupyter-julia :session jl :exports both
using Turing: Variational
Variational.elbo(advi, q, m, 1000)
#+end_src

#+RESULTS:
: -33.54428894047941

#+begin_src jupyter-julia :session jl
using LinearAlgebra

using Turing: Variational

using Bijectors: CouplingLayer, PartitionMask, Shift, Scale

using Flux
using Flux: @treelike, Optimise

@treelike CouplingLayer
@treelike Composed
@treelike Shift
@treelike Scale
@treelike TransformedDistribution

# TODO: NF-ADVI
var_info = Turing.VarInfo(m);
sym2range = Dict()
ranges = []
dists = []

start = 0

for k in keys(var_info.metadata)
    md = var_info.metadata[k]
    sym_ranges = []
    
    for i in 1:length(md.dists)
        d = md.dists[i]
        r = md.ranges[i]

        push!(sym_ranges, r .+ start)

        push!(dists, d)
        push!(ranges, r .+ start)
        start += r[end]
    end

    sym2range[k] = sym_ranges
end

sym2range

bs = inv.(bijector.(tuple(dists...)))
sb = Stacked(bs, ranges)

# TODO: Define NF
num_latent = ranges[end][end]
num_Œ∏ = length(vcat(collect.(sym2range[:Œ∏])...))
b_parameterized = RadialLayer(num_latent)


b = (sb ‚àò b_parameterized)
base = MvNormal(zeros(num_latent), ones(num_latent))
flow = transformed(base, b)

rand(flow)

# Coupling flow
nn_Œ∏2œÑ = Chain(Dense(1, 8, relu), Dense(8, 2 * num_Œ∏))
mask_Œ∏2œÑ = PartitionMask(
    num_latent,
    vcat(collect.(sym2range[:Œ∏])...),
    vcat(collect.(sym2range[:œÑ])...),
    vcat(collect.(sym2range[:Œº])...)
)
cl_Œ∏2œÑ = CouplingLayer(Œ∏ -> Affine(exp.(Œ∏[1:8]) .+ 0.1, Œ∏[9:16]), mask_Œ∏2œÑ, nn_Œ∏2œÑ)

nn_Œ∏2Œº = Chain(Dense(1, 8, relu), Dense(8, 2 * num_Œ∏))
mask_Œ∏2Œº = PartitionMask(
    num_latent,
    vcat(collect.(sym2range[:Œ∏])...),
    vcat(collect.(sym2range[:Œº])...),
    vcat(collect.(sym2range[:œÑ])...)
)

cl_Œ∏2Œº = CouplingLayer(Œ∏ -> Affine(exp.(Œ∏[1:8]) .+ 0.1, Œ∏[9:16]), mask_Œ∏2Œº, nn_Œ∏2Œº)

# Trying it out
cl_Œ∏2Œº(rand(flow))
(cl_Œ∏2œÑ ‚àò cl_Œ∏2Œº)(rand(flow))

# Defining the actual flow
b = sb ‚àò cl_Œ∏2Œº ‚àò cl_Œ∏2œÑ
base = Turing.Core.TuringDiagNormal(zeros(num_latent), ones(num_latent))
flow = transformed(base, b)

# check that everything works
logpdf(flow, rand(flow, 10))

x, y, logjac, logq = forward(flow, 3)

# Affine transform

Affine(W, b; dim::Type{Val{N}} = Val{1}) where {N} = Shift(b; dim = Val{N}) ‚àò Scale(W; dim = Val{N})

ŒºÃÇ = param(randn(num_latent) ./ sqrt(num_latent))
logœÉÃÇ = param(randn(num_latent) ./ sqrt(num_latent))


# So objective is nothing more than
elbo(q, logjoint, logjac) = mean(logjoint + logjac) + entropy(q)

opt = Variational.DecayedADAGrad(1e-4)
num_samples = 10
num_iters = 10_000

using OnlineStats
elbo_mean = Mean(weight = ExponentialWeight(1e-4))

using ProgressMeter
prog = ProgressMeter.Progress(num_iters)

for epoch = 1:num_iters
    aff = Affine(exp.(logœÉÃÇ), ŒºÃÇ)
    b = sb ‚àò cl_Œ∏2Œº ‚àò cl_Œ∏2œÑ ‚àò aff
    flow = transformed(base, b)
    
    _, y, logjac, _ = forward(flow, num_samples)
    logjoint = Tracker.collect([Variational.logdensity(m, var_info, y[:, i]) for i = 1:num_samples])
    
    obj = - elbo(flow, logjoint, logjac)
    Tracker.back!(obj, 1.0)
    # @info Tracker.data(obj)

    ‚àá_norm = 0.0

    # update params
    ‚àá_norm += norm(Tracker.grad(ŒºÃÇ))
    Optimise.update!(opt, ŒºÃÇ, Tracker.grad(ŒºÃÇ))
    ‚àá_norm += norm(Tracker.grad(logœÉÃÇ))
    Optimise.update!(opt, logœÉÃÇ, Tracker.grad(logœÉÃÇ))

    for p in Flux.params(cl_Œ∏2œÑ)
        ‚àá_norm += norm(Tracker.grad(p))
        Optimise.update!(opt, p, Tracker.grad(p))
    end

    for p in Flux.params(cl_Œ∏2Œº)
        ‚àá_norm += norm(Tracker.grad(p))
        Optimise.update!(opt, p, Tracker.grad(p))
    end

    fit!(elbo_mean, -Tracker.data(obj))
    ProgressMeter.next!(
        prog;
        showvalues = [
            (:elbo, -Tracker.data(obj)),
            (:elbo_mean, value(elbo_mean)),
            (:‚àá_norm, ‚àá_norm)]
    )
end

zÃÇ = mean(Tracker.data(rand(flow, 100)); dims = 2)
log(zÃÇ[1])

zs = hcat([Tracker.data(rand(flow, 1000)) for i = 1:10]...)
histogram(log.(zs[1, :]))

Œ∏_samples = zs[vec(collect.(sym2range[:Œ∏])...), :]
mean(Œ∏_samples; dims = 2)

histogram(Œ∏_samples[1, :])
#+end_src

* Combining EVERYTHING
** Use case
- Suppose you have a large dataset $\mathcal{Z} = \left\{ z_i \right\}_{i = 1}^m$
- Then, in some other setting, maybe just a different experiment, you get observations $\mathcal{X} = \left\{ x_i \right\}_{i = 1}^n$ for some other random variable $x$
- You have prior knowledge that indeed $x$ depends on $z$, but you're not sure what prior to use for $z$

** Example: selling ice cream /optionally/
Example: you have two /different/ datasets
- $\mathcal{Z} := \left\{ z_j \right\}_{j = 1}^m$, locations of the j-th person in some country ‚Üí $p(z)$ is a "population density"
- $\mathcal{X} := \left\{ x_i \right\}_{i = 1}^n$, integer labels corresponding to the ice cream vendor from which the i-th person purchased ice cream
- $m$ and $n$ are not necessarily the same, and these datasets are obtained in two different scenarios, maybe two different days

You own a series of $k$ ice cream parlours on the beach, but this is not enough for you. You want to expand your business into also selling fake sunglasses. You want your sellers to annoyingly walk around and ask people if they want these sunglasses, but obviously you want to make this as efficient as possible.

You have a dataset $\mathcal{X} = \left\{ x_i \right\}_{i = 1}^N$ of ice creams sold, and, you were quite lucky on this one, you found a hacker which could sell you GPS data $\mathcal{Z} = \left\{ z_j \right\}_{j = 1}^M$ of all the people on the beach! Unfortunately this was just a single snapshot of the locations of all the $M$ people on the beach at a single time, but this was in the middle of the day so you think it is a reasonable representation of the location-density.

Unfortunately you have /a lot/ of GPS data but not so much ice cream data. What you would really like to ask is "where are people located when they go and buy ice cream?", or rather, "what is $p(z \mid x = 1)$?". If you knew this, then you could put your on-the-move napkin-salesmen in those high-probability-of-purchasing-ice-cream locations! This way you can be there when they all inevitably need napkins after eating their ice-cream. 


*** Approach
1. Density estimation using $\mathcal{Z}$ to get a population density $p(z)$
2. Define model for ice cream purchases
   \begin{equation*}
   \begin{split}
     z & \sim \hat{p}(z \mid \mathcal{Z}) \\
     \pi & := f(z) \\
     x_i & \sim \mathrm{Categorical}(\pi), \quad i = 1, \dots, n
   \end{split}
   \end{equation*}
   where $f$ maps location $z$ to probabilities over the different ice cream parlors on the beach.
3. Perform exact inference using a MCMC method, e.g. =ParticleGibbs=


** How
- Maybe you have some highly complex data for which you don't have, nor want to make any, assumptions
- Can do density estimation of the data using a normalizing flow!

#+begin_src jupyter-julia :session jl
d = 10

dist = MvNormal(zeros(d), ones(d))
b = PlanarFlow(d) ‚àò RadialFlow(d) ‚àò PlanarFlow(d) ‚àò RadialFlow(d)
flow = transformed(dist, b)

# TODO: do density estimation using `flow`

# TODO: define a model which uses the learned `flow`
@model flow_based(x) = begin
    z ~ flow

    for i = size(x, 2)
        x[:, i] ~ MvNormal(z, ones(d))
    end
end


# TODO: generate data
m = flow_based(data)
samples = sample(m, HMC(1000, 0.1, 10); progress = true)
#+end_src


