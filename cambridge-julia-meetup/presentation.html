<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>You have data and I have distributions: a talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></title>
<meta name="author" content="(Tor Erlend Fjelde)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//css/reveal.css"/>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//css/theme/moon.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5//MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link href="https://fonts.googleapis.com/css?family=Quicksand" rel="stylesheet">
<script defer src="https://use.fontawesome.com/releases/v5.0.9/js/all.js" integrity="sha384-8iPTk2s/jMVj81dnzb/iFR2sdA7u06vHJyyLlAd4snFpCl/SnyUjRrbdJsw1pGIl" crossorigin="anonymous"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><div><h1>You have data and I have distributions</h1><h3>A talk on <code>Turing.jl</code> and <code>Bijectors.jl</code></h3><div style="margin: -200px auto; opacity: 0.2;"><p><object data="https://turing.ml/dev/assets/images/turing-logo-wide.svg"></object></p></div></div>
</section>

<section>
<section id="slide-org033abe2">
<h2 id="org033abe2">Overview</h2>
<ol>
<li class="fragment appear">Bayesian inference
<ul>
<li>Why you want to do Bayesian inference</li>
<li>What it means to do Bayesian inference</li>

</ul></li>
<li class="fragment appear"><code>Turing.jl</code> on a simple example
<ul>
<li>Bayesian inference</li>
<li><i>Approximate</i> Bayesian inference (variational inference)</li>

</ul></li>
<li class="fragment appear"><code>Bijectors.jl</code>:
<ul>
<li>What it's about</li>
<li>Why it's neat</li>
<li>Normalising flow</li>

</ul></li>
<li class="fragment appear">Combining <i>everything</i></li>

</ol>

</section>
</section>
<section>
<section id="slide-orgf7c5769">
<h2 id="orgf7c5769">Bayesian inference and such</h2>
<ul>
<li class="fragment appear">Have some dataset \(\left\{ x_i \right\}_{i = 1}^n\)</li>
<li class="fragment appear">Believe data \(\left\{ x_i \right\}_{i = 1}^n\) was generated by some process and we're interested in inferring parameters \(\theta\) of this process
<ul>
<li>E.g. in linear regression you have data \(\left\{ (x_i, y_i) \right\}_{i = 1}^n\) and want to infer coefficients \(\theta := \beta\)</li>

</ul></li>

</ul>

</section>
<section id="slide-orgf7c5769-split">

<p>
<span class="underline">Frequentist</span>
</p>
<blockquote nil>
<p>
A single point as the estimate of \(\theta\) is good enough.
</p>
</blockquote>

<div class="fragment (appear)">

<p>
<span class="underline">Bayesian</span> 
</p>
<blockquote nil>
<p>
But we have <span class="underline">finite</span> data! And this dataset just happen to give you the one and only \(\theta\)?!
</p>

<p>
No, no, no, we need a distribution over the \(\theta\) given the data (a <b>posterior</b>):
</p>
<div>
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big)
\end{equation*}

</div>
</blockquote>

</div>

</section>
<section id="slide-org87ea1a8">
<h3 id="org87ea1a8">Bayes' rule</h3>
<p>
<b>Bayes' rule</b> gives us
</p>
<div>
\begin{equation*}
p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n\big) = \frac{p\big(\left\{ x_i \right\}_{i = 1}^n \mid \theta\big) p(\theta)}{p\big(\left\{ x_i \right\}_{i = 1}^n\big)}
\end{equation*}

</div>
<div class="fragment (appear)">
<p>
or, since the denominator is constant,
</p>
<div>
\begin{equation*}
\begin{split}
  p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big) &\propto p \big( \left\{ x_i \right\}_{i = 1}^n \mid \theta \big) p(\theta) \\
  &= p \big( \theta, \left\{ x_i \right\}_{i = 1}^n \big)
\end{split}
\end{equation*}

</div>
</div>

<div class="fragment (appear)">
<p>
For the family of inference methods known as <i>Markov Chain Monte-Carlo (MCMC)</i>, this proportional factor is all we need.
</p>
</div>

</section>
</section>
<section>
<section id="slide-orgd7d3dfb">
<h2 id="orgd7d3dfb">Setup</h2>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Pkg; Pkg.activate(<span style="color: #98C379;">"."</span>)
</code></pre>
</div>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Plots, StatsPlots
</code></pre>
</div>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Bijectors
</code></pre>
</div>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Turing
</code></pre>
</div>

<p>
<span class="underline">Disclaimer:</span> All functionality in this talk is not yet available on the master branch of <code>Bijectors.jl</code>, but should be soon™.
</p>

</section>
<section id="slide-orgd7d3dfb-split">

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>versioninfo()
</code></pre>
</div>

<pre class="example">
Julia Version 1.1.1
Commit 55e36cc308 (2019-05-16 04:10 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.1 (ORCJIT, skylake)

</pre>

</section>
</section>
<section>
<section id="slide-orga9e7f73">
<h2 id="orga9e7f73"><code>Turing.jl</code></h2>
<blockquote  class="fragment (appear)">
<p>
<code>Turing.jl</code> is a (universal) <span class="underline">probabilistic programming language</span> (PPL) in Julia.
</p>
</blockquote>

<p class="fragment (appear)">
What does that even mean?
</p>

<ol>
<li class="fragment appear">Specify generative model in Julia with neat syntax</li>
<li class="fragment appear">Bayesian inference to estimate posterior \(p\big(\theta \mid \left\{ x_i \right\}_{i = 1}^n \big)\) using a vast library of MCMC samplers</li>
<li class="fragment appear">???</li>
<li class="fragment appear">Profit (in expectation)!!!</li>

</ol>

</section>
<section id="slide-org22d18b7">
<h3 id="org22d18b7">Example: Gaussian-InverseGamma conjugate model</h3>
<p class="fragment (appear)">
In mathematical notation:
</p>
<div class="fragment (appear)">
\begin{equation*}
\begin{split}
  s & \sim \mathrm{InverseGamma}(2, 3) \\
  m & \sim \mathcal{N}(0, \sqrt{s}) \\
  x_i & \sim \mathcal{N}(m, \sqrt{s}), \quad i = 1, \dots, n
\end{split}
\end{equation*}

</div>

<p class="fragment (appear)">
In <code>Turing.jl</code>:
</p>
<div class="org-src-container">

<pre  class="fragment (appear)"><code trim><span style="color: #828997;">@model</span> model(x) = <span style="color: #C678DD;">begin</span>
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, &#8730;s)
    <span style="color: #C678DD;">for</span> i = 1:length(x)
        x[i] ~ Normal(m, &#8730;s)
    <span style="color: #C678DD;">end</span>
<span style="color: #C678DD;">end</span>
</code></pre>
</div>

</section>
<section id="slide-org22d18b7-split">

<p>
Generate some fake data and instantiate the model
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>xs = randn(1_000)
m = model(xs)
</code></pre>
</div>

<div class="fragment (appear)">

<p>
Now sample to obtain posterior \(p\big(m, s \mid \left\{ x_i \right\}_{i = 1}^n \big)\)
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #5C6370;"># </span><span style="color: #5C6370;">Sample 1000 samples using HMC</span>
samples_nuts = sample(m, NUTS(10_000, 200, 0.65));
</code></pre>
</div>

</div>

<div class="fragment (appear)">

<p>
Aaaand we can plot the resulting (empirical) posterior
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>plot(samples_nuts[[<span style="color: #828997;">:s</span>, <span style="color: #828997;">:m</span>]])
</code></pre>
</div>

</div>

</section>
<section id="slide-org22d18b7-split">


<div class="figure">
<p><object type="image/svg+xml" data="figures/gaussian-inversegamma-hmc.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section id="slide-org333d90d">
<h3 id="org333d90d"><b>Approximate</b> inference (Variational inference)</h3>
<p>
Might be happy with an <i>approximation</i> to your posterior \(p \big( \theta \mid \left\{ x_i \right\}_{i = 1}^n \big)\).
</p>

<div class="fragment (appear)">

<p>
<b>Variational inference (VI)</b> is an approximate approach which formulates the problem as an <i>optimization</i> problem:
</p>
</div>
<div class="fragment (appear)">
<div>
\begin{equation*}
\underset{q \in \mathscr{Q}}{\text{argmin}}\ \mathrm{D_{KL}} \big( q(\theta) \mid p(\theta \mid \left\{ x_i \right\}_{i = 1}^n ) \big) \quad \text{or} \quad \underset{q \in \mathscr{Q}}{\text{argmax}}\ \mathrm{ELBO} \big( q(\theta) \big)
\end{equation*}

</div>
<p>
where
</p>
<div>
\begin{equation*}
\mathrm{ELBO} \big( q(\theta) \big) = \mathbb{E}_{\theta \sim q(\theta)} \big[ \log p\big(\theta, \left\{ x_i \right\}_{i = 1}^n \big) \big] + \mathbb{H} \big( q(\theta) \big)
\end{equation*}

</div>

</div>

<p class="fragment (appear)">
<span class="underline">Caveat:</span> usually assume \(\mathscr{Q}\) is the family of Gaussians with <i>diagonal</i> covariance.
</p>

</section>
<section id="slide-org78ec9c2">
<h4 id="org78ec9c2">Automatic Differentiation Variational Inference (ADVI)</h4>
<p>
(Mean-field) ADVI is a simple but flexible VI approach that exists in <code>Turing.jl</code>
</p>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #5C6370;"># </span><span style="color: #5C6370;">"Configuration" for ADVI</span>
<span style="color: #5C6370;"># </span><span style="color: #5C6370;">- 10 samples for gradient estimation</span>
<span style="color: #5C6370;"># </span><span style="color: #5C6370;">- Perform 15 000 optimization steps</span>
advi = ADVI(10, 15_000)

<span style="color: #5C6370;"># </span><span style="color: #5C6370;">Perform `ADVI` on model `m` to get variational posterior `q`</span>
q = vi(m, advi)
</code></pre>
</div>

<div class="fragment (appear)">
<p>
To sample and compute probabilities
</p>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>xs = rand(q, 10)
logpdf(q, xs)
</code></pre>
</div>
</div>

</section>
<section id="slide-org78ec9c2-split">


<div class="figure">
<p><img src="./figures/advi_w_elbo_fps15_125_forward_diff.gif" alt="advi_w_elbo_fps15_125_forward_diff.gif" />
</p>
<p><span class="figure-number">Figure 2: </span><code>ADVI</code> applied to the Normal-InverseGamma generative model from earlier. <span class="underline">Disclaimer:</span> this plot is generated by writing the optimization loop myself rather than using the simple <code>vi(m, advi)</code> call. See <code>test/skipped/advi_demo.jl</code> in <code>Turing.jl</code> for the code used.</p>
</div>

</section>
<section id="slide-org0acbd04">
<h3 id="org0acbd04">Benchmarks of <code>HMC</code></h3>

<div class="figure">
<p><img src="figures/turing-benchmarks.png" alt="turing-benchmarks.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-orgaeeba0c">
<h2 id="orgaeeba0c"><code>Bijectors.jl</code></h2>
<div class="fragment (appear) definition" id="def:bijector">
<p>
A <b>bijector</b> or <b>diffeomorphism</b> is a differentiable <i>bijection</i> \(b\) with a <i>differentiable</i> inverse \(b^{-1}\).
</p>

</div>

<p class="fragment (appear)">
For example \(b(x) = \exp(x)\)
</p>
<ul>
<li class="fragment appear">\(\exp\) is differentiable</li>
<li class="fragment appear">\(\exp\) has inverse \(\log\)</li>
<li class="fragment appear">\(\log\) is differentiable (on \((0, \infty)\))</li>

</ul>
<p class="fragment (appear)">
So \(\exp\) (and \(\log\)) is a bijector!
</p>

</section>
<section id="slide-orgaeeba0c-split">

<p>
In <code>Bijectors.jl</code>
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Bijectors; <span style="color: #C678DD;">using</span> Bijectors: Exp, Log

b = Exp()
b&#8315;&#185; = inv(b)

b&#8315;&#185; isa Log
</code></pre>
</div>

<pre class="example">
true

</pre>

<div class="fragment (appear)">
<p>
We can evaluate a <code>Bijector</code>
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>x = 0.0
b(x) == 1.0  <span style="color: #5C6370;"># </span><span style="color: #5C6370;">since e&#8304; = 1</span>
</code></pre>
</div>

<pre class="example">
true

</pre>

</div>

<div class="fragment (appear)">
<p>
We can <i>compose</i> bijectors to get a new <code>Bijector</code>
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>(b &#8728; b) isa Bijector
</code></pre>
</div>

<pre class="example">
true

</pre>

</div>

</section>
<section id="slide-orgaeeba0c-split">

<p>
And evaluate compositions of bijectors
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>(b&#8315;&#185; &#8728; b)(x) == x
</code></pre>
</div>

<pre class="example">
true

</pre>

<p class="fragment (appear)">
What about more complex/deeper compositions?
</p>

<div class="fragment (appear)">
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>cb = b &#8728; b &#8728; b
cb&#8315;&#185; = inv(cb)        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&lt;= inversion of a "large" composition</span>

(cb&#8315;&#185; &#8728; cb)(x) == x
</code></pre>
</div>

<pre class="example">
true

</pre>

</div>

<div class="fragment (appear)">
<p>
We'll see later that of particular interest is the term
</p>

<div>
\begin{equation*}
\log \left| \det \mathcal{J}_{b^{-1}}(y) \right| \quad \text{or} \quad \log \left| \det \mathcal{J}_{b}(x) \right|
\end{equation*}

</div>
</div>

<div class="fragment (appear)">
<p>
Which works seamlessly even for compositions
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>logabsdetjac(cb, x)
</code></pre>
</div>

<pre class="example">
3.718281828459045

</pre>

</div>

</section>
<section id="slide-orgc27bbcf">
<h3 id="orgc27bbcf">How does it relate to distributions?</h3>
<div class="fragment (appear)">
<p>
Consider
</p>
<div>
\begin{equation*}
x \sim \mathcal{N}(1, 5)
\end{equation*}

</div>
</div>
<div class="fragment (appear)">
<p>
Or, equivalently,
</p>
<div>
\begin{equation*}
\begin{split}
  \xi &\sim \mathcal{N}(0, 1) \\
  x &:= 1 + 5 \xi
\end{split}
\end{equation*}

</div>
</div>

</section>
<section id="slide-orgc27bbcf-split">

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>d1 = Normal(1.0, 5.0) <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&#119977;(1, 5)</span>
xs = rand(d1, 100_000)

d2 = Normal(0.0, 1.0) <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&#119977;(0, 1)</span>
&#958; = rand(d2, 100_000)
ys = 1.0 .+ &#958; .* 5.0  <span style="color: #5C6370;"># </span><span style="color: #5C6370;">y ~ &#119977;(1, 5)</span>

density(xs, label = <span style="color: #98C379;">"d1"</span>, linewidth = 3)
density!(ys, label = <span style="color: #98C379;">"d2"</span>, linewidth = 3)
</code></pre>
</div>


<div class="figure">
<p><object type="image/svg+xml" data="figures/normal-pdfs.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section id="slide-orgc27bbcf-split">

<p>
<code>Bijector</code> + <code>Distribution</code> = another <code>Distribution</code>
</p>

<div class="fragment (appear)">
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Bijectors: Shift, Scale

<span style="color: #5C6370;"># </span><span style="color: #5C6370;">Define the transform</span>
b = Shift(1.0) &#8728; Scale(5.0)           <span style="color: #5C6370;"># </span><span style="color: #5C6370;">=&gt; x &#8614; 1.0 + 5.0x</span>
td = transformed(Normal(0.0, 1.0), b) <span style="color: #5C6370;"># </span><span style="color: #5C6370;">=&gt; &#119977;(1.0, 5.0)</span>
</code></pre>
</div>

</div>

<p class="fragment (appear)">
Moreover, <code>td</code> is a <code>TransformedDistribution</code> <i>and</i>
</p>

<div class="fragment (appear)">
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>td isa Distribution
</code></pre>
</div>

<pre class="example">
true

</pre>

<p>
Yay!
</p>

</div>

<div class="fragment (appear)">

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>y = rand(td)
<span style="color: #5C6370;"># </span><span style="color: #5C6370;">Ensure we have the same densities</span>
logpdf(td, y) &#8776; logpdf(d1, y)
</code></pre>
</div>

<pre class="example">
true

</pre>

</div>

</section>
<section id="slide-orgc27bbcf-split">

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>x_range = -14.0:0.05:16.0
plot(x_range, x -&gt; pdf(Normal(1.0, 5.0), x), label = <span style="color: #98C379;">"d1"</span>, linewidth = 4, linestyle = <span style="color: #828997;">:dash</span>)
plot!(x_range, x -&gt; pdf(td, x), label = <span style="color: #98C379;">"td"</span>, linewidth = 4, alpha = 0.6)
</code></pre>
</div>


<div class="figure">
<p><object type="image/svg+xml" data="figures/normal-transformed-pdfs.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 5: </span>Density of \(\mathcal{N}(0, 1)\) transformed by \(\xi \mapsto 1 + 5\xi\) (labeled <code>td</code>) and \(\mathcal{N}(1, 5)\) (labeled <code>d1</code>).</p>
</div>

</section>
<section id="slide-orgc27bbcf-split">

<blockquote nil>
<p>
Great; just another way to draw samples from a normal distribution&#x2026;
</p>
</blockquote>

<p class="fragment (appear)">
Or <i>is it</i>?
</p>

<div class="org-src-container">

<pre  class="fragment (appear)"><code trim><span style="color: #C678DD;">using</span> Random; Random.seed!(10); <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&lt;= chosen because of nice visualization</span>
d = MvNormal(zeros(1), ones(1))

<span style="color: #5C6370;"># </span><span style="color: #5C6370;">"Deep" transformation</span>
b = (
    RadialLayer(1) &#8728; RadialLayer(1) &#8728;
    RadialLayer(1) &#8728; RadialLayer(1)
)
td = transformed(d, b)

<span style="color: #5C6370;"># </span><span style="color: #5C6370;">Sample from flow</span>
xs = rand(td, 10_000)

x_range = minimum(xs):0.05:maximum(xs)
lps = pdf(td, reshape(x_range, (1, :)))  <span style="color: #5C6370;"># </span><span style="color: #5C6370;">compute the probabilities</span>

histogram(vec(xs), bins = 100; normed = <span style="color: #D19A66;">true</span>, label = <span style="color: #98C379;">""</span>, alpha = 0.7)
xlabel!(<span style="color: #98C379;">"y"</span>)
plot!(x_range, lps, linewidth = 3, label = <span style="color: #98C379;">"p(y)"</span>)
</code></pre>
</div>

</section>
<section id="slide-orgc27bbcf-split">

<div class="figure">
<p><object type="image/svg+xml" data="figures/simple-nf-pdf.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<p>
That doesn't look very <i>normal</i>, now does it?!
</p>

</section>
<section id="slide-orgf8ec021">
<h4 id="orgf8ec021">How?</h4>
<p>
It turns out that if \(b\) is a <code>Bijector</code>, the process
</p>
<div>
\begin{equation*}
\begin{split}
  x & \sim p \\
  y & := b(x)
\end{split}
\end{equation*}

</div>
<p>
<i>induces</i> a density \(\tilde{p}(y)\) defined by
</p>
<div>
\begin{equation*}
\tilde{p}(y) := p \big( b^{-1}(y) \big) \left| \det \mathcal{J}_{b^{-1}}(y) \right|
\end{equation*}

</div>

<div class="fragment (appear)">
<p>
Therefore if we can compute \(\mathcal{J}_{b^{-1}}(y)\) we can indeed compute \(\tilde{p}(y)\)!
</p>
</div>

<blockquote  class="fragment (appear)">
<p>
But is this actually useful?
</p>
</blockquote>

</section>
<section id="slide-org028ff9f">
<h3 id="org028ff9f">Normalising flows: parameterising \(b\)</h3>
<p>
One might consider constructing a <i>parameterised</i> <code>Bijector</code> \(b_{\phi}\).
</p>

<div class="fragment (appear)">
<p>
Given a density \(p(x)\) we can obtain a parameterised density
</p>
<div>
\begin{equation*}
\tilde{p}_{\phi}(y) = p \big( b_{\phi}^{-1}(y) \big) \left| \det \mathcal{J}_{b_{\phi}^{-1}}(y) \right|
\end{equation*}

</div>
</div>

<blockquote  class="fragment (appear)">
<p>
\(b_{\phi}\) is often referred to as a <b>normalising flow (NF)</b>
</p>
</blockquote>

<div class="fragment (appear)">
<p>
Can now optimise any objective over distributions, e.g. perform <i>maximum likelihood estimation</i> (MLE) for some given i.i.d. dataset \(\left\{ y_i \right\}_{i = 1}^n\)
</p>
<div>
\begin{equation*}
\underset{\phi}{\text{argmax}}\ \sum_{i = 1}^{n} \tilde{p}_{\phi}(y_i)
\end{equation*}

</div>
</div>

</section>
<section id="slide-org74b01d0">
<h4 id="org74b01d0">Example: MLE using NF</h4>
<p>
Consider an <code>Affine</code> transformation, i.e.
</p>
<div>
\begin{equation*}
\mathrm{aff}(x) = W x + b
\end{equation*}

</div>
<p>
for matrix \(W\) and vector \(b\),
</p>
<div class="fragment (appear)">
<p>
and a non-linear (but <i>invertible</i>) activation function, e.g. <code>LeakyReLU</code>
</p>
<div>
\begin{equation*}
a(x) = 
\begin{cases}
  x & \text{if } x \ge 0 \\
  \alpha x & \text{if } x < 0
\end{cases}
\end{equation*}

</div>
<p>
for some <i>non-zero</i> \(\alpha \in \mathbb{R}\) (usually chosen to be very small).
</p>
</div>

<p class="fragment (appear)">
Can define a "deep" NF by composing such transformations! Looks familiar?
</p>

<div class="fragment (appear)">
<p>
Yup; it's basically an invertible neural network! (assuming \(\det W \ne 0\))
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>layers = [LeakyReLU(&#945;[i]) &#8728; Affine(W[i], b[i]) <span style="color: #C678DD;">for</span> i = 1:num_layers]

b = foldl(&#8728;, layers)
td = transformed(base_dist, b)  <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&lt;= "deep" normalising flow!</span>
</code></pre>
</div>

</div>

</section>
<section id="slide-org74b01d0-split">


<div class="figure">
<p><img src="figures/nf-banana-density-estimation.gif" alt="nf-banana-density-estimation.gif" width="35%" />
</p>
<p><span class="figure-number">Figure 7: </span>Empirical density estimate (blue) compared with single batch of samples (red). Code can be found in <code>scripts/nf_banana.jl</code>.</p>
</div>

</section>
<section id="slide-orge2a5033">
<h3 id="orge2a5033">Methods</h3>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Operation</th>
<th scope="col" class="org-left">Method</th>
<th scope="col" class="org-left">Automatic</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(b \mapsto b^{-1}\)</td>
<td class="org-left"><code>inv(b)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\((b_1, b_2) \mapsto (b_1 \circ b_2)\)</td>
<td class="org-left"><code>b1 ∘ b2</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\((b_1, b_2) \mapsto [b_1, b_2]\)</td>
<td class="org-left"><code>stack(b1, b2)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\((b, n) \mapsto b^n := b \circ \cdots \circ b\) (n times)</td>
<td class="org-left"><code>b^n</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-orge2a5033-split">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Operation</th>
<th scope="col" class="org-left">Method</th>
<th scope="col" class="org-left">Automatic</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(x \mapsto b(x)\)</td>
<td class="org-left"><code>b(x)</code></td>
<td class="org-left">\(\times\)</td>
</tr>

<tr>
<td class="org-left">\(y \mapsto b^{-1}(y)\)</td>
<td class="org-left"><code>inv(b)(y)</code></td>
<td class="org-left">\(\times\)</td>
</tr>

<tr>
<td class="org-left">\(x \mapsto \log \lvert\det \mathcal{J}_b(x)\rvert\)</td>
<td class="org-left"><code>logabsdetjac(b, x)</code></td>
<td class="org-left">AD</td>
</tr>

<tr>
<td class="org-left">\(x \mapsto \big( b(x), \log \lvert \det \mathcal{J}_b(x)\rvert \big)\)</td>
<td class="org-left"><code>forward(b, x)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-orge2a5033-split">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Operation</th>
<th scope="col" class="org-left">Method</th>
<th scope="col" class="org-left">Automatic</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(p \mapsto q:= b_* p\)</td>
<td class="org-left"><code>q = transformed(p, b)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\(y \sim q\)</td>
<td class="org-left"><code>y = rand(q)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\(p \mapsto b\) s.t. \(\mathrm{support}(b_* p) = \mathbb{R}^d\)</td>
<td class="org-left"><code>bijector(p)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>

<tr>
<td class="org-left">\(\big(x \sim p, b(x), \log \lvert\det \mathcal{J}_b(x)\rvert, \log q(y) \big)\)</td>
<td class="org-left"><code>forward(q)</code></td>
<td class="org-left">\(\checkmark\)</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org86529e5">
<h3 id="org86529e5">Implementing a <code>Bijector</code></h3>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> StatsFuns: logit, logistic

<span style="color: #C678DD;">struct</span> <span style="color: #E5C07B;">Logit</span>{T<span style="color: #ABB2BF; background-color: #282C34;">&lt;:</span><span style="color: #E5C07B;">Real</span>} <span style="color: #ABB2BF; background-color: #282C34;">&lt;:</span> <span style="color: #E5C07B;">Bijector</span>{0} <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&lt;= 0-dimensional, i.e. expects `Real` input (or `Vector` which is treated as batch)</span>
    a<span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">T</span>
    b<span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">T</span>
<span style="color: #C678DD;">end</span>

(b<span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">Logit</span>)(x) = @. logit((x - b.a) / (b.b - b.a))
<span style="color: #5C6370;"># </span><span style="color: #5C6370;">`orig` contains the `Bijector` which was inverted</span>
(ib<span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">Inversed</span>{<span style="color: #ABB2BF; background-color: #282C34;">&lt;:</span><span style="color: #E5C07B;">Logit</span>})(y) = @. (ib.orig.b - ib.orig.a) * logistic(y) + ib.orig.a

<span style="color: #61AFEF;">logabsdetjac</span>(b<span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">Logit</span>, x) = @. - log((x - b.a) * (b.b - x) / (b.b - b.a))
</code></pre>
</div>

</section>
<section id="slide-org86529e5-split">

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>julia&gt; b = Logit(0.0, 1.0)
Logit{<span style="color: #E5C07B;">Float64</span>}(0.0, 1.0)

julia&gt; y = b(0.6)
0.4054651081081642

julia&gt; inv(b)(y)
0.6

julia&gt; logabsdetjac(b, 0.6)
1.4271163556401458

julia&gt; logabsdetjac(inv(b), y) <span style="color: #5C6370;"># </span><span style="color: #5C6370;">defaults to `- logabsdetjac(b, inv(b)(x))`</span>
-1.4271163556401458

julia&gt; forward(b, 0.6)         <span style="color: #5C6370;"># </span><span style="color: #5C6370;">defaults to `(rv=b(x), logabsdetjac=logabsdetjac(b, x))`</span>
(rv = 0.4054651081081642, logabsdetjac = 1.4271163556401458)
</code></pre>
</div>

</section>
</section>
<section>
<section id="slide-org0012703">
<h2 id="org0012703">NF-ADVI vs. MF-ADVI</h2>
<div class="fragment (appear)">

<p>
Consider \(L = \begin{pmatrix} 10 & 0 \\ 10 & 10 \end{pmatrix}\) and
</p>
<div>
\begin{equation*}
\begin{split}
  m & \sim \mathcal{N}(0, 1) \\
  x_i & \overset{i.i.d.}{=} \mathcal{N}(m, L L^T) \quad i = 1, \dots, n
\end{split}
\end{equation*}

</div>

</div>

<div class="fragment (appear)">

<p>
In <code>Turing.jl</code>
</p>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #C678DD;">using</span> Turing

L = [
    10 0;
    10 10
]

<span style="color: #828997;">@model</span> demo(x) = <span style="color: #C678DD;">begin</span>
    &#956; ~ MvNormal(zeros(2), ones(2))

    <span style="color: #C678DD;">for</span> i = 1:size(x, 2)
        x[:, i] ~ MvNormal(&#956;, L * transpose(L))
    <span style="color: #C678DD;">end</span>
<span style="color: #C678DD;">end</span>
</code></pre>
</div>

</div>

</section>
<section id="slide-org0012703-split">

<p>
Generate <code>n = 100</code> samples with true mean <code>μ = [0.0, 0.0]</code>
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #5C6370;"># </span><span style="color: #5C6370;">Data generation</span>
n = 100
&#956;_true = 0.5 .* ones(2)  <span style="color: #5C6370;"># </span><span style="color: #5C6370;">&lt;= different from original problem</span>
likelihood = MvNormal(&#956;_true, L * transpose(L))
xs = rand(likelihood, n)
</code></pre>
</div>

</section>
<section id="slide-org0012703-split">

<div id="mvnormal-comparison">


<div class="figure">
<p><img src="figures/mvnormal-1-posterior.png" alt="mvnormal-1-posterior.png" />
</p>
<p><span class="figure-number">Figure 8: </span>True posterior</p>
</div>


<div class="figure">
<p><img src="figures/mvnormal-1-mfvi-elbo.png" alt="mvnormal-1-mfvi-elbo.png" />
</p>
<p><span class="figure-number">Figure 9: </span>MF-ADVI</p>
</div>

<div class="fragment (appear)">


<div class="figure">
<p><img src="figures/mvnormal-1-nfvi-elbo.png" alt="mvnormal-1-nfvi-elbo.png" />
</p>
<p><span class="figure-number">Figure 10: </span>NF-ADVI (rational-quadratic)</p>
</div>


<div class="figure">
<p><img src="figures/mvnormal-1-nfvi-elbo-affine.png" alt="mvnormal-1-nfvi-elbo-affine.png" />
</p>
<p><span class="figure-number">Figure 11: </span>NF-ADVI (affine)</p>
</div>

</div>

</div>

</section>
</section>
<section>
<section id="slide-org4b075cf">
<h2 id="org4b075cf">Combining <span class="underline">EVERYTHING</span></h2>
<div class="outline-text-2" id="text-org4b075cf">
</div>
</section>
<section id="slide-org659d140">
<h3 id="org659d140">Real-world example: selling napkins like a professional</h3>
<ul>
<li class="fragment appear">You own <code>k</code> ice-cream parlours on a beach</li>
<li class="fragment appear">Business is going well → want to expand &amp; diversify</li>
<li class="fragment appear"><span class="underline">Genius idea #1:</span> <i>on-the-move napkin salesmen</i></li>

</ul>

</section>
<section id="slide-org659d140-split">

<ul>
<li class="fragment appear">Issue #1: what should the "napkin-route" be?</li>
<li class="fragment appear">You do have observations of which parlour people bought ice-cream at
<ul>
<li>E.g. \(x_i = 1\) if the i-th customer bought ice cream at Parlour #1</li>

</ul></li>
<li class="fragment appear"><span class="underline">Genius idea #2:</span> Bayesian inference to get \(p\big(\text{location} \mid \left\{ x_i \right\}_{i = 1}^n \big)\)
<ul>
<li>Use posterior to decide where to define the napkin-route</li>
<li><p>
Generative model is
</p>
<div>
\begin{equation*}
\begin{split}
  \mathrm{loc}_i &\sim p(\text{location}) \\
  \pi_i &:= f(\mathrm{loc}_i) \in \mathbb{R}^{k} \\
  x_i &\sim \mathrm{Categorical}(\pi_i), \quad i = 1, \dots, n
\end{split}
\end{equation*}

</div>
<p>
where \(f\) maps the location to some probability vector.
</p></li>

</ul></li>
<li class="fragment appear">Issue #2: need a prior for beach-people's locations</li>
<li class="fragment appear"><span class="underline">Genius idea #3:</span> get hacker friend to hack everyone's phones to obtain location data, then perform density estimation on this location data to get a <i>prior</i> over the locations.</li>

</ul>


</section>
<section id="slide-org659d140-split">


<div class="figure">
<p><object type="image/svg+xml" data="figures/ice-cream/beach_annotated.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 12: </span>Beach plot. <code>locations</code> (yellow) refer to samples from the location prior (density estimated using normalising flow). The blue part represents the sea, and the black part represents land which is <i>not</i> part of the beach so we don't care about it.</p>
</div>

</section>
<section id="slide-org659d140-split">


<div class="figure">
<p><object type="image/svg+xml" data="figures/ice-cream/prior_10000_1000.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 13: </span>Beach prior. 6-layer NF with <code>Affine</code> and <code>LeakyReLU</code> as seen in animation from before.</p>
</div>


</section>
<section id="slide-org94345f1">
<h4 id="org94345f1">Observations &amp; model</h4>
<p>
Generate fake data (most people buy from Parlour #1)
</p>
<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim>fake_samples = [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1]
num_fake_samples = length(fake_samples)
</code></pre>
</div>

<div class="fragment (appear)">

<p>
Define a <code>Model</code> which uses a NF as prior
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #828997;">@model</span> napkin_model(x, <span style="color: #ABB2BF; background-color: #282C34;">::</span><span style="color: #E5C07B;">Type</span>{TV} = <span style="color: #E5C07B;">Vector</span>{<span style="color: #E5C07B;">Float64</span>}) <span style="color: #C678DD;">where</span> {TV} = <span style="color: #C678DD;">begin</span>
    locs = <span style="color: #E5C07B;">Vector</span>{TV}(<span style="color: #D19A66;">undef</span>, length(x))

    <span style="color: #C678DD;">for</span> i &#8712; eachindex(x)
        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">We sample from the original distribution then transform to help NUTS.</span>
        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">Could equivalently have done `locs[i] ~ td` but more difficult to sample from.</span>
        locs[i] ~ td.dist
        loc = td.transform(locs[i])

        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">Compute a notion of "distance" from `loc` to the two different ice-cream parlours</span>
        d1 = exp(- norm(parlour1 - loc))
        d2 = exp(- norm(parlour2 - loc))

        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">The closer `loc` is to a ice-cream parlour, the more likely customer at `loc`</span>
        <span style="color: #5C6370;"># </span><span style="color: #5C6370;">will buy from that instead of the other.</span>
        &#960;s = [d1 / (d1 + d2), d2 / (d1 + d2)]

        x[i] ~ Categorical(&#960;s)
    <span style="color: #C678DD;">end</span>
<span style="color: #C678DD;">end</span>
</code></pre>
</div>

</div>

</section>
<section id="slide-org94345f1-split">

<p>
Bayesian inference time
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-julia"><code trim><span style="color: #5C6370;"># </span><span style="color: #5C6370;">Instantiate model</span>
m = napkin_model(fake_samples)

<span style="color: #5C6370;"># </span><span style="color: #5C6370;">Sample using NUTS</span>
num_mcmc_samples = 10_000
mcmc_warmup = 1_000
samples = sample(m, NUTS(mcmc_warmup, 0.65), num_mcmc_samples);
</code></pre>
</div>

<div class="fragment (appear">


<div class="figure">
<p><object type="image/svg+xml" data="figures/ice-cream/posterior_10000_1000.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 14: </span>Beach posterior.</p>
</div>

</div>

</section>
<section id="slide-org94345f1-split">

<p>
Guaranteed great success. 
</p>

<p>
A couple of years later you're probably already in the business of bribing politicians to reduce worker-benefits for napkin-salesmen.
</p>

</section>
</section>
<section>
<section id="slide-orgc13abad">
<h2 id="orgc13abad">Thank you</h2>
<div><div><code>TuringLang</code> (website): <a href="https://turing.ml">https://turing.ml</a></div><div><code>TuringLang</code> (Github): <a href="https://github.com/TuringLang">https://github.com/TuringLang</a></div><div><code>Turing.jl</code> (Github): <a href="https://github.com/TuringLang/Turing.jl">https://github.com/TuringLang/Turing.jl</a></div><div><code>Bijectors.jl</code> (Github): <a href="https://github.com/TuringLang/Bijectors.jl">https://github.com/TuringLang/Bijectors.jl</a></div></div>
</section>
</section>
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.8.0//plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
